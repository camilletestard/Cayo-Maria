rm(list = ls())# clear all variables
data <- read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Social_Cog_X_Social_Network/GazeData_Networks_noNAN_3trial.csv") # include only individuals who completed 3 trials
names(data)[1]<-"ID"
bmASR <- glm(Look ~ Sex+ Age + Trial * ord.rank , data = data, family = binomial)
summary(bmASR)
bmASR <- glm(Look ~ Sex+ Age + Trial * percent.rank , data = data, family = binomial)
summary(bmASR)
rm(list = ls())# clear all variables
data <- read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Social_Cog_X_Social_Network/GazeData_Networks_noNAN_4trial.csv") # include only individuals who completed 3 trials
names(data)[1]<-"ID"
bmASR <- glm(Look ~ Sex+ Age + Trial * percent.rank , data = data, family = binomial)
summary(bmASR)
# Load all trials data:
rm(list = ls())# clear all variables
data <- read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Social_Cog_X_Social_Network/GazeData_Networks_noNAN.csv")
names(data)[1]<-"ID"
str(data)
bmSAR <- glmer(Look ~ Trial + Sex + Age + percent.rank +(1|ID), data = data, family = binomial)
summary(bmSAR)
bmSAR <- glmer(Look ~ Sex + Age + Trial*percent.rank +(1|ID), data = data, family = binomial)
summary(bmSAR)
rm(list = ls())# clear all variables
data <- read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Social_Cog_X_Social_Network/GazeData_Networks_noNAN_4trial.csv") # include only individuals who completed 3 trials
names(data)[1]<-"ID"
bmASR <- glm(Look ~ Sex+ Age + Trial * ord.rank , data = data, family = binomial)
summary(bmASR)
bmR <- glm(Look ~ Trial * ord.rank , data = data, family = binomial)
summary(bmR)
bmASR <- lm(Latency ~ Sex+ Age + Trial * percent.rank , data = data)
summary(bmASR)
rm(list = ls())# clear all variables
data <- read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Social_Cog_X_Social_Network/GazeData_Networks_noNAN.csv")
names(data)[1]<-"ID"
str(data)
bmASR <- lmer(Latency ~ Sex+ Age + Trial * percent.rank + (1|ID), data = data)
summary(bmASR)
#Looking at the effect of rank on individuals who completed the same number of trials
rm(list = ls())# clear all variables
data <- read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Social_Cog_X_Social_Network/GazeData_Networks_noNAN_3trial.csv") # include only individuals who completed 3 trials
names(data)[1]<-"ID"
bmR <- glm(Look ~ Trial * ord.rank , data = data, family = binomial)
summary(bmR)
bmSAG <- glmer(Look ~ Sex + Age + Trial*z.G.S + (1|ID), data = data, family = binomial)
summary(bmSAG)
bmSARG <- glmer(Look ~ Sex + Age + percent.rank + Trial*z.G.S + (1|ID), data = data, family = binomial)
summary(bmSARG)
bmSAP <- glmer(Look ~ Sex + Age + percent.rank +Trial*n.social.partners + (1|ID), data = data, family = binomial)
summary(bmSAP)
# Load all trials data:
rm(list = ls())# clear all variables
data <- read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Social_Cog_X_Social_Network/GazeData_Networks_SEA_noNAN.csv")
names(data)[1]<-"ID"
# load required libraries
library(lme4)
library(afex)
bmSAP <- glmer(Look ~ Sex + Age + percent.rank +Trial*n.social.partners + (1|ID), data = data, family = binomial)
summary(bmSAP)
bmSAW <- glmer(Look ~ Sex + Age + percent.rank + Trial*n.weak.connections + (1|ID), data = data, family = binomial)
summary(bmSAW)
bmSAC <- glmer(Look ~ Sex + Age + percent.rank + Trial*CompositeSocialityIndex + (1|ID), data = data, family = binomial)
summary(bmSAC)
bmSAT <- glmer(Look ~ Sex + Age + percent.rank + Trial*CSI.to.Top3 + (1|ID), data = data, family = binomial)
summary(bmSAT)
data = read.table('C:\Users\Camille Testard\Desktop\Behavioral_Ecology\Research_projects\Behavior_DataFiles2016\GroupHH2016_AgonisticActions.txt')
data = read.table('C:\Users\Camille Testard\Desktop\Behavioral_Ecology\Research_projects\Behavior_DataFiles2016\GroupHH2016_AgonisticActions.txt', header = TRUE, sep=",")
data = read.table(":\Users\Camille Testard\Desktop\Behavioral_Ecology\Research_projects\Behavior_DataFiles2016\GroupHH2016_AgonisticActions.txt", header = TRUE, sep=",")
data = read.table(":/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_AgonisticActions.txt", header = TRUE, sep=",")
data = read.table(":/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_AgonisticActions.txt")
data = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_AgonisticActions.txt")
data = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_AgonsiticActions.txt")
View(data)
View(data)
View(data)
View(data)
data = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_AgonsiticActions.txt", header=TRUE)
View(data)
data = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_AgonsiticActions.txt", header=TRUE, sep=",")
View(data)
data = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_FocalData.txt", header=TRUE, sep=",")
View(data)
View(data)
data = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_GroomingEvents.txt", header=TRUE, sep=",")
View(data)
rm(list = ls())# clear all variables
#Load data
data_grooming = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_GroomingEvents.txt", header=TRUE, sep=",")
data_proximity = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_ProximityGroups.txt", header=TRUE, sep=",")
View(data_proximity)
View(data_grooming)
data_group = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_GroupByYear.txt", header=TRUE, sep=",")
View(data_grooming)
View(data_grooming)
View(data_grooming)
View(data_group)
View(data_group)
install.packages("igraph", dependencies = TRUE)
install.packages("sna", dependencies = TRUE)
install.packages("tnet", dependencies = TRUE)
library(igraph)
library(sna)
library(tnet)
#Need to upload an adjacency matrix, rather than socprog style data...
#read adjacency matrix
data=read.csv("C:\Users\Camille Testard\Desktop\Behavioral_Ecology\Research_projects\Behavior_DataFiles2016\GroupHH2016_grooming_adjacency_matrix.csv",header=TRUE,row.names=1,check.names=FALSE) # choose an adjacency matrix from a .csv file
data=read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_grooming_adjacency_matrix.csv",header=TRUE,row.names=1,check.names=FALSE) # choose an adjacency matrix from a .csv file
rm(list = ls())# clear all variables
data=read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_grooming_adjacency_matrix.csv",header=TRUE,row.names=1,check.names=FALSE) # choose an adjacency matrix from a .csv file
View(data)
m=as.matrix(data)
View(m)
View(m)
demo(package="igraph")
help("igraph")
am.g=graph.adjacency(m,mode="directed",weighted=T)
View(am.g)
View(am.g)
am.g
A.Weightdeg<-degree(data)
ls()
A.Weightdeg
A.Weight.IN.deg <-degree(data, gmode="digraph", diag=FALSE, tmaxdev=FALSE, cmode="indegree", rescale=FALSE, ignore.eval=FALSE)
#weighted indegree
A.Weight.IN.deg <-degree(data, gmode="digraph", diag=FALSE, tmaxdev=FALSE, cmode="indegree", rescale=FALSE, ignore.eval=FALSE) #digraph = directed graph, cmode=type of degree centrality
#weighted outdegree
A.Weight.OUT.deg <-degree(data, gmode="digraph", diag=FALSE, tmaxdev=FALSE, cmode="outdegree", rescale=FALSE, ignore.eval=FALSE)
#Binary degree (inDegree)
A.Bindeg<-apply(data,2,function(a)sum(a>0))
A.Boutdeg<-apply(data,1,function(a)sum(a>0))
A.Bindeg
A.Boutdeg<-apply(data,1,function(a)sum(a>0))
rm(list = ls())# clear all variables
data <- read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Social_Cog_X_Social_Network/GazeData_Networks_noNAN_3trial.csv") # include only individuals who completed 3 trials
names(data)[1]<-"ID"
# two-way mixed ANOVA (both within and between subject)
aov2 <- aov(Look ~ Trial * ord.rank + Error(ID/Trial), data = data)
summary(aov2)
aov2 <- aov(Look ~ Trial * ord.rank, data = data)
summary(aov2)
install.packages("ggpubr", dependencies = TRUE)
xbar <- 159
n <- 1047
ns <- 100
M = matrix(rbinom(n*ns, size=1, prob=xbar/n), nrow=n)
#Load data
data_grooming = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_GroomingEvents.txt", header=TRUE, sep=",")
data_proximity = read.table("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_ProximityGroups.txt", header=TRUE, sep=",")
#C
View(data_proximity)
data_proximity.in.proximity(10)
data_proximity$in.proximity(10)
data_proximity$in.proximity
data_proximity$in.proximity[10]
View(data_proximity)
rm(list = ls())# clear all variables
library(igraph)
library(sna)
library(tnet)
#Need to upload an adjacency matrix, rather than socprog style data...
#read adjacency matrix
data=read.csv("C:/Users/Camille Testard/Desktop/Behavioral_Ecology/Research_projects/Behavior_DataFiles2016/GroupHH2016_CSI_adjacency_matrix.csv",header=TRUE,row.names=1,check.names=FALSE) # choose an adjacency matrix from a .csv file
m=as.matrix(data) # coerces the data set as a matrix
am.g=graph.adjacency(m,mode="directed",weighted=T) # this will create an directed 'igraph object'. Change qualifiers to make "undirected" or unweighted (null)
am.g
View(am.g)
#Get the network measures
#Weighted degree (Strength, undirected)
A.Weightdeg<-degree(data) #
A.Weightdeg
A.Weight.IN.deg <-degree(data, gmode="digraph", diag=FALSE, tmaxdev=FALSE, cmode="indegree", rescale=FALSE, ignore.eval=FALSE)
A.Weight.OUT.deg <-degree(data, gmode="digraph", diag=FALSE, tmaxdev=FALSE, cmode="outdegree", rescale=FALSE, ignore.eval=FALSE)
A.Weight.IN.deg
A.Weight.OUT.deg
A.Bindeg<-apply(data,2,function(a)sum(a>0))
A.Boutdeg<-apply(data,1,function(a)sum(a>0))
A.Clust<-transitivity(am.g,"local")
#clustering coeff (local). How close are the neighbors of a graph to be a clique (a complete graph). Are your friends also friends between each other?
A.Clust<-transitivity(am.g,"local") #from igraph package
#Weighted betweenness.The number of shortest paths that pass through the vertex.
A.Weight.between<-betweenness(data,gmode="graph",ignore.eval=F)
#Weighted eigenvector centrality
A.Weight.eig.cen<-evcent(data,gmode="graph",ignore.eval=F)
A.Weight.eig.cen<-evcent(data,gmode="graph",ignore.eval=F) #from 'sna' package
networkMeasures<-data.frame(cbind(A.Weightdeg,A.Weight.IN.deg,A.Weight.OUT.deg, A.Bindeg, A.Boutdeg, A.Clust,A.Weight.between,A.Weight.eig.cen))
write.table(networkMeasures, file = "A.2016HH_Centrality_Output.csv", sep = ",", col.names=T, row.names=T)
View(networkMeasures)
View(networkMeasures)
vcount(am.g)
vcount(am.g)^2.3
vcount(am.g)^2.8
V(am.g)
V(am.g)$label.cex <- 0.5
#increase space between nodes if overlapping
#fruchterman reingold layout
l <- layout.fruchterman.reingold(am.g,niter=500,area=vcount(am.g)^2.3,repulserad=vcount(am.g)^2.8)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.5
View(l)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name*2, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name*0.5, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name*0.5, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
V(am.g)$name
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.5,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
#fruchterman reingold layout
l <- layout.fruchterman.reingold(am.g,niter=500,area=vcount(am.g)^3.3,repulserad=vcount(am.g)^3.8)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.5
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
vcount(am.g)
vcount(am.g)^3.3
l <- layout.fruchterman.reingold(am.g,niter=500,area=vcount(am.g)^2,repulserad=vcount(am.g)^2.3)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.5
#plot graph
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
#spring embedded layout
s <- layout.spring(am.g, spring.length=1000,area=vcount(am.g)^2.3,repulserad=vcount(am.g)^2.8)
plot.igraph(am.g,layout=l, vertex.label=NA, vertex.color="orange", vertex.size=6,edge.color="grey20",edge.width=E(am.g)$weight*0.01)
plot.igraph(am.g,layout=l, vertex.label=NA, vertex.color="orange", vertex.size=6,edge.color="grey20",edge.width=E(am.g)$weight*0.01, edge.arrow.size = 0.2)
View(networkMeasures)
A.Weight.IN.deg
A.Weightdeg
A.Weight.OUT.deg
A.Weight.between
A.Weight.between
A.Weight.between<-betweenness(data,gmode="digraph",ignore.eval=F)
A.Weight.between
A.Weight.closenessn<-closeness(data,gmode="digraph",ignore.eval=F) #from 'sna' package
A.Weight.closeness<-closeness(data,gmode="digraph",ignore.eval=F) #from 'sna' package
A.Weight.closeness
A.Weight.closeness<-closeness(data,gmode="digraph",ignore.eval=F)
A.Weight.closeness
closeness(data,gmode="digraph",ignore.eval=F)
evcent(data,gmode="graph",ignore.eval=F)
closeness(data,gmode="graph",ignore.eval=F)
A.Weight.eig.cen<-evcent(data,gmode="digraph",ignore.eval=F)
A.Weight.eig.cen
A.Weight.eig.cen<-evcent(data,gmode="graph",ignore.eval=F)
A.Weight.eig.cen
install.packages("Perc", dependencies = TRUE)
install.packages("fitdistrplus", dependencies = TRUE)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/ModelEffects.RData")
hist(NotAloneEffects$isPost)
hist(SocialEffects$isPost)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/ModelEffects.RData")
hist(NotAloneEffects$isPost)
hist(SocialEffects$isPost)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/ModelEffects.RData")
install.packages("gdata",dependencies = T)
keep(c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects"))
gdata::keep(c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects"))
c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects")
rm(list=setdiff(ls(),c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects")))
hist(NotAloneEffects$isPost)
hist(NotAloneEffects$isPost,add =F)
hist(SocialEffects$isPost,add =T)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/ModelEffects.RData")
rm(list=setdiff(ls(),c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects")))
#1. Show the distribution of the Hurricane effect on P(Acc) and P(Social) considering all data:
hist(NotAloneEffects$isPost,add =F)
hist(NotAloneEffects$isPost,add =F)
hist(SocialEffects$isPost,add =TRUE)
library(ggplot2)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/AllStats3.RData")
AllStats[["KK.2013.1"]]= NULL
groupyear = c("V.2015","V.2016","V.2017","KK.2015","KK.2017")
gy=1
paste(c("GlobalNetworkMetrics",groupyear[gy],"pdf"),sep = ".")
paste("GlobalNetworkMetrics",groupyear[gy],"pdf",sep = ".")
groupyear = c("V.2015","V.2016","V.2017","KK.2015","KK.2017")
for (gy in 1:length(groupyear)){ #For each group
name.0 = paste(groupyear[gy],".0",sep="")
data.0 = AllStats[[name.0]]; data.0$isPost = 0
name.1 = paste(groupyear[gy],".1",sep="")
data.1 = AllStats[[name.1]]; data.1$isPost = 1
data= rbind(data.0, data.1)
pdf(file= paste("GlobalNetworkMetrics",groupyear[gy],"pdf",sep = "."), width=5, height=5, onefile = T) #width and height of the graphics region in inches. One file if true = multiple graphs in one file
#hist(data.0$dens); hist(data.1$dens)
{density <- ggplot(data, aes(x= as.factor(isPost), y=dens, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Density of social network pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Density of social network")}
#ylim(0,0.4)
# print(density)
# readline(prompt = "pause ")
#hist(data.0$gini); hist(data.1$gini)
{equity <- ggplot(data, aes(x= as.factor(isPost), y=gini, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Equity of social network pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Gini coeff relative to baseline")}
#ylim(0,0.9)
# print(equity)
# readline(prompt = "pause ")
}
library(ggplot2)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/AllStats4.RData")
##########################################################
#Sex partner preference
##########################################################
groupyear = c("V.2015","V.2016","V.2017","KK.2015","KK.2017")
for (gy in 1:length(groupyear)){ #For each group
name.0 = paste(groupyear[gy],".0",sep="")
data.0 = AllStats[[name.0]]; data.0$isPost = 0
name.1 = paste(groupyear[gy],".1",sep="")
data.1 = AllStats[[name.1]]; data.1$isPost = 1
data= rbind(data.0, data.1)
pdf(file= paste("SexPartnerPref",groupyear[gy],"pdf",sep = "."), width=5, height=5, onefile = T)
{FFpair <- ggplot(data, aes(x= as.factor(isPost), y=eo.FF, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("FF Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. FF")}
#ylim(0,0.4)
# print(FFpair)
# readline(prompt = "pause ")
{MMpair <- ggplot(data, aes(x= as.factor(isPost), y=eo.MM, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("MM Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. MM")}
#ylim(0,0.4)
# print(MMpair)
# readline(prompt = "pause ")
{crosspair <- ggplot(data, aes(x= as.factor(isPost), y=eo.cross, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Cross Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. Cross")}
#ylim(0,0.4)
# print(crosspair)
# readline(prompt = "pause ")
}
##########################################################
#Kinship preference
##########################################################
for (gy in 1:length(groupyear)){ #For each group
name.0 = paste(groupyear[gy],".0",sep="")
data.0 = AllStats[[name.0]]; data.0$isPost = 0
name.1 = paste(groupyear[gy],".1",sep="")
data.1 = AllStats[[name.1]]; data.1$isPost = 1
data= rbind(data.0, data.1)
pdf(file= paste("KinshipPartnerPref",groupyear[gy],"pdf",sep = "."), width=5, height=5, onefile = T)
CKpair <- ggplot(data, aes(x= as.factor(isPost), y=eo.ck, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Close Kin Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. Close Kin")
#ylim(0,0.4)
print(CKpair)
readline(prompt = "pause ")
DKpair <- ggplot(data, aes(x= as.factor(isPost), y=eo.dk, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Distant Kin Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. Distant Kin")
#ylim(0,0.4)
print(DKpair)
readline(prompt = "pause ")
Upair <- ggplot(data, aes(x= as.factor(isPost), y=eo.u, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Unrelated Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. Unrelated")
#ylim(0,0.4)
print(Upair)
readline(prompt = "pause ")
}
library(lme4)# Generalized Linear Mixed Models
library(lmerTest)
library(performance)
#library(sjPlot)
#library(glmmTMB)# Generalized Linear Mixed Models, other package
#library(MCMCglmm)# Generalized Linear Mixed Models, other package
#library(bbmle)#Tools for General Maximum Likelihood Estimation
#library(DHARMa) #residual diagnostic fr hierarchical (multi-level/mixed) regression models
#Load data
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/SocialCapital.dSocialRates.RData")
#Format data correclty
SocialCapital.ALL$sex = as.factor(SocialCapital.ALL$sex); SocialCapital.ALL$age = as.numeric(SocialCapital.ALL$age); SocialCapital.ALL$group = as.factor(SocialCapital.ALL$group)
data = SocialCapital.ALL[-which(is.na(SocialCapital.ALL$dpAcc)),] #remove NA
#check distribution of independent variables
hist(data$dpAcc); hist(data$dpSocial, add=T)
#Scale parameters:
data[,c("age","GroomIN","GroomOUT","AggIN","AggOUT","vig.ra","sdb.ra")] <- scale(data[,c("age","GroomIN","GroomOUT","AggIN","AggOUT","vig.ra","sdb.ra")])
## Model Social Capital effect on change in sociliaty rates
#Modelling change in p(Acc)
dpAcc1 <- lmer(dpAcc~ sex + age + group + rank + (1|id) + (1|year), data = data)
summary(dpAcc1)
tab_model(dpAcc1)
install.packages("tsna",dependencies = T)
install.packages("ndtv",dependencies = T)
#load required packages
library(network)
library(ergm)
library(dplyr)
# library(sna)
library(igraph)
# library(tnet)
library(stringr)
library(ggplot2)
#load local functions
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/cleaned_code/functions")
source("CalcSubsampledScans.R")
source("functions_Homophily.R")
source("KinshipPedigree.R")
#Load scan data, population and dominance info
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2019.txt")
dominance_info =read.table("Behavioral_Data/Database Complete/Data All Raw/DOMINANCE.txt",header = T)
bigped <- read.delim("Behavioral_Data/SubjectInfo_2010-2017/PEDIGREE.txt", sep="\t")
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/")
load("R.Data/SocialCapital.RData")
#Compute pedigree for all IDs in this group
allIDs= allScans$focalID[which(allScans$group == "KK"|allScans$group == "V")];
groupIDs = as.character(unique(allIDs))
IDmatch = match(groupIDs, as.character(bigped$ID)); discard.na = which(is.na(IDmatch))
if(length(discard.na)!=0)
{pedigree = bigped[IDmatch[-discard.na],c("ID","DAM","SIRE")]
} else {pedigree = bigped[IDmatch,c("ID","DAM","SIRE")]}
ped <- KinshipPedigree(pedigree)
actions = c("groom", "prox")
num_iter = 500
newP.homophily.all=data.frame()
start_time <- Sys.time(); iter=1; a=1
# 1. Calculate random subsamples
randomScans = calcRandomScans(allScans)
#Create age category for later
age.thresh = as.numeric(quantile(randomScans$age,0.7, na.rm=T))
randomScans$age.cat="Y"; randomScans$age.cat[randomScans$age >= age.thresh]="O"
# 2. For each group, each year and pre-/post-hurr separately, compute weighted edge list:
group = c("V","KK"); g=1; Networks = data.frame(); #Make sure t
randscansG = randomScans[which(randomScans$group==group[g]),] #subselect scans of group G
years = unique(randscansG$year); y=1
randscansY = randscansG[which(randscansG$year==years[y]),] #subselect scans of group G and year Y
year = years[y]
isPost = c(0,1); h=1
rscans = randscansY[which(randscansY$isPost==isPost[h]),] #subselect scans of group G, year Y and hurricane status H
numscans = as.data.frame(table(as.character(rscans$focalID))); names(numscans) =c("id","freq")
#Find all unique IDs
unqIDs = unique(as.character(rscans$focalID))
# Output the Master Edgelist of all possible pairs given the unique IDs.
if (actions[a] == "prox") {masterEL = calcMasterEL(unqIDs)}
if (actions[a] == "groom") {masterEL = calcMasterEL_groom(unqIDs)}
# Output weighted edgelist from the Master Edgelist.
options(warn = -1) #set options to ignore all warnings
if (actions[a] == "prox") {weightedEL = calcEdgeList(rscans,masterEL)}
if (actions[a] == "groom") {weightedEL = calcEdgeList_groom(rscans,masterEL)
weightedEL$alter = weightedEL$givingID; weightedEL$ego = weightedEL$receivingID
#IMPORTANT NOTE: alter = giving ID and ego = receiving ego. Important for later.
weightedEL$givingID <- NULL; weightedEL$receivingID <-NULL
weightedEL = weightedEL[,c("alter","ego","count")]
weightedEL$conc = paste(weightedEL$alter, weightedEL$ego, sep=".")
}
weightedEL$numscans <- (numscans$freq[match(weightedEL$alter, numscans$id)] + numscans$freq[match(weightedEL$ego, numscans$id)])/2
weightedEL$weight <- round(weightedEL$count / weightedEL$numscans, 5) #add weight information by dividing by avg #observations for each ID pair
meanweight = mean(weightedEL$weight[weightedEL$weight>0]) #compute nonzero mean weight
weightedEL$weight <- weightedEL$weight/meanweight
# weightedEL = weightedEL[which(weightedEL$weight != 0),]#only keep nonzero weighted edges
weightedEL$iter = iter; weightedEL$group = group[g]; weightedEL$year = years[y]; weightedEL$isPost = isPost[h]; weightedEL$action = actions[a]
#   #Save network
#   Networks = rbind(Networks, weightedEL)# row bind
#
# } #end of isPost for loop
# data = Networks[which(Networks$group == group[g] & Networks$year == years[y]),]
# data.0 = data[which(data$isPost == 0),]; data.1 = data[which(data$isPost == 1),] #create two structures containing pre-hurricane data (data.0) and post-hurricane data
#
# #Find new partners
# newP = data.1[which(is.na(match(data.1$conc, data.0$conc))),] #new pairs = pairs that were not in pre-hurricane network
el= weightedEL[,c("alter","ego",'weight'),]
#Transform into matrix
adjMat = dils::AdjacencyFromEdgelist(el)# create adjacency matrix based on edge list.
data = adjMat[["adjacency"]]; rownames(data) = adjMat[["nodelist"]]; colnames(data) = adjMat[["nodelist"]]
#read in example network and create
# setwd("C:/Users/Camille Testard/Desktop/Analyzing_social_networks")
# b<-read.csv("ergmtoynet.csv")
# names<-b[,1]
# b<-as.matrix(b[,2:ncol(b)])
# rownames(b)<-colnames(b)<-names
b<-data

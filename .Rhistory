library(xlsx)
install.packages("xlsx")
library(xlsx)
mydata <- read.table("/Users/camilletestard/Desktop/Social_Cog_X_Social_Network/GazeData_Networks.csv")
View(mydata)
View(mydata)
mydata <- read.table("/Users/camilletestard/Desktop/Social_Cog_X_Social_Network/GazeData_Networks.csv", header = TRUE, sep = ",", row.names = "id")
mydata <- read.table("/Users/camilletestard/Desktop/Social_Cog_X_Social_Network/GazeData_Networks.csv", header = TRUE, sep = ",")
attach(mydata)
plot(Total_looking, percent.rank)
plot(percent.rank, Total_looking)
abline(lm(Total_looking-percent.rank))
plot(percent.rank, Total_looking)
library(rJava)
library(xlsx)
install.packages(c("rJava", "xlsx"))
library(rJava)
library(cluster)
library(class)
library(nnet)
library(xlsx)
library(rJava)
?foo
?aabline
?abline
?lm
attach(mydata)
plot(percent.rank, Total_looking)
abline(lm(Total_looking~percent.rank))
mydata(pos = 6)
attach(mydata)
plot(percent.rank, Total_looking)
mydata <- read.table("/Users/camilletestard/Desktop/Social_Cog_X_Social_Network/GazeData_Networks.csv", header = TRUE, sep = ",")
# Plot data
attach(mydata)
plot(percent.rank, Total_looking)
?ploy
?plot
plot(percent.rank, Total_looking, "p")
plot(percent.rank, Total_looking, "p")
plot(percent.rank, Total_looking, "p")
plot(Total_looking, percent.rank, "p")
lm(Total_looking~percent.rank)
abline(lm(Total_looking~percent.rank))
plot(z.G.S, z.G.InS, "p")
plot(z.G.S, z.G.InS, "p")
ok <- ! is.na(subset$z.G.S)
attach(mydata)
ok <- ! is.na(subset$z.G.S)
plot(z.G.S, z.G.InS, "p", subset )
abline(lm(Total_looking~percent.rank))
View(mydata)
View(mydata)
dim(mydata)
plot(mydata$Age)
plot(sex)
plot(mydata$Sex)
plot(z.G.S)
plot(mydata$Age, z.G.S)
plot(mydata$Age, z.G.S)
plot(Age, z.G.S)
View(mydata)
View(mydata)
plot(mydata$Sex)
View(mydata)
attach(mydata)
plot(percent.rank, z.G.S)
abline(lm(z.G.S~percent.rank))
plot(percent.rank, z.G.S)
sample(100,100)
sample(100,100)/100
prob.pre<-sample(100,100)/100
prob.post<-sample(100,100)/100
change <- prob.post-prob.pre
plot(prob.pre, change)
prob.pre<-sample(20,100)/100
prob.post<-sample(20,100)/100
change <- prob.post-prob.pre
plot(prob.pre, change)
prob.pre<-sample(20,100, T)/100
prob.post<-sample(20,100, T)/100
change <- prob.post-prob.pre
plot(prob.pre, change)
hit(change)
hist(change)
prob.post<-sample(20,1000, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change)
prob.pre<-sample(20,1000, T)/100
prob.post<-sample(20,1000, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change)
plot(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(20,100, T)/100
prob.post<-sample(20,100, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change)
cor.test(prob.pre, change)
prob.pre<-sample(20,100, T)/100
prob.post<-sample(100,100, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(20,100, T)/100
prob.post<-sample(30,100, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(10,100, T)/100
prob.post<-sample(40,100, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(10,100, T)/100
prob.post<-sample(30,100, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(10,100, T)/100
prob.post<-sample(10,100, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(20,100, T)/100
prob.post<-sample(20,100, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(20,100, T)/100
prob.post<-sample(80,100, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(20,80, T)/100
prob.post<-sample(80,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(20,80, T)/100
prob.post<-sample(60,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(15,80, T)/100
prob.post<-sample(40,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(30,80, T)/100
prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<-sample(25,80, T)/100
prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
a.pre = 0.01;
b.pre = 0.05;
prob.pre<- a.pre.*randn(100,1) + b.pre;
a.pre = 0.01;
b.pre = 0.05;
a.pre.*randn(100,1) + b.pre
a.pre*randn(100,1) + b.pre
rand(100,1)
prob.pre<- rnorm(80, mean=0.05, sd=0.01);
# prob.pre<- sample(1,80, T)/100
prob.pre<- rnorm(80, mean=0.2, sd=0.01);
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
ist(prob.pre)
hist(prob.pre)
prob.pre<- sample(1,80, T)/100
hist(prob.pre)
prob.pre<- sample(100,80, T)/100
hist(prob.pre)
prob.pre<- rnorm(80, mean=0.05, sd=0.01);
hist(prob.pre)
prob.post<- rnorm(80, mean=0.15, sd=0.01);
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
prob.pre<- rnorm(80, mean=0.1, sd=0.01);
# prob.pre<- sample(100,80, T)/100
hist(prob.pre)
prob.post<- rnorm(80, mean=0.5, sd=0.01);
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<- rnorm(80, mean=0.1, sd=0.01);
# prob.pre<- sample(100,80, T)/100
hist(prob.pre)
prob.post<- rnorm(80, mean=0.5, sd=0.01);
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
prob.pre<- rnorm(80, mean=0.2, sd=0.01);
prob.pre
rnorm(80, mean=0.05, sd=0.01);
prob.pre<- rnorm(80, mean=0.1, sd=0.1);
# prob.pre<- sample(100,80, T)/100
hist(prob.pre)
prob.post<- rnorm(80, mean=0.1, sd=0.1);
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<- rnorm(80, mean=0.1, sd=0.1);
# prob.pre<- sample(100,80, T)/100
prob.post<- rnorm(80, mean=0.5, sd=0.1);
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<- abs(rnorm(80, mean=0.1, sd=0.1))
abs(rnorm(80, mean=0.1, sd=0.1))
prob.pre<- abs(rnorm(80, mean=0.1, sd=0.1))
prob.post<- abs(rnorm(80, mean=0.5, sd=0.1))
prob.post
prob.post<- abs(rnorm(80, mean=0.5, sd=0.2));
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
prob.pre<- abs(rnorm(80, mean=0.1, sd=0.1));
# prob.pre<- sample(100,80, T)/100
prob.post<- abs(rnorm(80, mean=0.2, sd=0.2));
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
prob.pre<- abs(rnorm(80, mean=0.1, sd=0.1));
# prob.pre<- sample(100,80, T)/100
prob.post<- abs(rnorm(80, mean=0.2, sd=0.2));
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
prob.pre<- abs(rnorm(80, mean=0.1, sd=0.1));
# prob.pre<- sample(100,80, T)/100
prob.post<- abs(rnorm(80, mean=0.2, sd=0.1));
# prob.post<-sample(100,80, T)/100
change <- prob.post-prob.pre
hist(change)
plot(prob.pre, change); cor.test(prob.pre, change)
abline(h= 0, add= T)
install.packages(c("ergm", "htmlwidgets", "latticeExtra", "ndtv", "network", "statnet"))
# edges = control for the change in degree
# gwesp = model whether there are more triangles than expected by chance for a network of this size and density, and thus
# that there is some sort of explicit triangle closure effect going on.
# mutual = test whether bond formation is more likely in the case of reciprocating a bond
# kinship & proximity edge covariate = test whether bond formation between two nodes is more or less likely to form as kin
# relationship or proximity increases
# Positive coeff = relationshis is more likely than chance to form (negative -> less likely)
# Bond dissolution model ~ edges + kinship edge cov.+ Proximity edge cov.
# Positive coeff = bond is more likely than chance to persist in the next time step.
# For more information:
#   Silk et al 2017 (Animal Behavior)
# http://statnet.org/Workshops/ergm_tutorial.html#appendix_a:_clarifying_the_terms_%E2%80%93_ergm_and_network
# http://statnet.org/Workshops/tergm_tutorial.html
#load required packages
library(network)
library(ergm)
library(dplyr)
library(stringr)
library(ggplot2)
library(statnet)
library(ndtv)
library(htmlwidgets)
library(latticeExtra)
# http://statnet.org/Workshops/ergm_tutorial.html#appendix_a:_clarifying_the_terms_%E2%80%93_ergm_and_network
# http://statnet.org/Workshops/tergm _tutorial.html
#load local functions
#setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/cleaned_code/functions")
setwd("Users/camilletestard/Documents/GitHub/Cayo-Maria/cleaned_code/functions")
source("CalcSubsampledScans.R")
source("functions_GlobalNetworkMetrics.R")
source("KinshipPedigree.R")
setwd("Users/camilletestard/Desktop/Desktop-Cayo-Maria/")
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans.txt")
#Load scan data, population and dominance info
#setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
setwd("Users/camilletestard/Desktop/Desktop-Cayo-Maria/")
#Load scan data, population and dominance info
#setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
setwd("/Users/camilletestard/Desktop/Desktop-Cayo-Maria/Behavioral_Data/")
allScans = read.csv("Data All Cleaned/allScans.txt")
#load local functions
#setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/cleaned_code/functions")
setwd("Users/camilletestard/Documents/GitHub/Cayo-Maria/cleaned_code/functions")
source("CalcSubsampledScans.R")
#load local functions
#setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/cleaned_code/functions")
setwd("/Users/camilletestard/Documents/GitHub/Cayo-Maria/cleaned_code/functions")
source("CalcSubsampledScans.R")
source("functions_GlobalNetworkMetrics.R")
source("KinshipPedigree.R")
# setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/")
setwd("/Users/camilletestard/Documents/GitHub/Cayo-Maria/")
load("R.Data/SocialCapital.RData")
SocialCapital.ALL$groupyear = paste(SocialCapital.ALL$group, SocialCapital.ALL$year,sep="")
#Create gregariousness category
threshold=as.numeric(quantile(SocialCapital.ALL$std.DSIgroom, probs = 0.70))
SocialCapital.ALL$groomCat="greg";
SocialCapital.ALL$groomCat[which(SocialCapital.ALL$std.DSIgroom<threshold)]="shy"
num_iter = 20
#For each group, each year separately:
group = c("V","V","V","KK","KK") #c("V","V","V","V","V","KK","KK","KK","S")
years = c(2015, 2016,2017,2015, 2017)#c(2015,2016,2017,2018, 2019, 2015, 2017, 2018, 2019)
groupyears =c("V2015","V2016","V2017","KK2015","KK2017") #c("V2015","V2016","V2017","V2018","V2019","KK2015","KK2017","KK2018", "S2019")
#Initilize dataframes, time and iteration variables.
TERGMeffects = data.frame(); TERGMeffects.ALL=data.frame(); count=0
start_time <- Sys.time(); iter=1; a=1; gy=1
# 1. Calculate random subsamples
randomScans = calcRandomScans(allScans);
randomScans$groupyear = paste(randomScans$group, randomScans$year,sep="")
# 2. For each group, each year and pre-/post-hurr separately, compute weighted edge list:
gy=3
randscansY = randomScans[which(randomScans$groupyear==groupyears[gy]),] #subselect scans of group G
isPost = c(0,1); h=1
rscans = randscansY[which(randscansY$isPost==isPost[h]),] #subselect scans of group G, year Y and hurricane status H
numscans = as.data.frame(table(as.character(rscans$focalID))); names(numscans) =c("id","freq")
#Find all unique IDs
unqIDs = unique(as.character(rscans$focalID))
# Output the Master Edgelist of all possible pairs given the unique IDs.
masterEL = calcMasterEL_groom(unqIDs)
# Output weighted edgelist from the Master Edgelist for grooming network
weightedEL.groom = calcEdgeList_groom(rscans,masterEL)
weightedEL.groom = weightedEL.groom[,c("givingID","receivingID","count")]
weightedEL.groom$conc = paste(weightedEL.groom$givingID, weightedEL.groom$receivingID, sep=".")
weightedEL.groom$numscans <- (numscans$freq[match(weightedEL.groom$givingID, numscans$id)] + numscans$freq[match(weightedEL.groom$receivingID, numscans$id)])/2
weightedEL.groom$weight <- round(weightedEL.groom$count / weightedEL.groom$numscans, 5) #add weight information by dividing by avg #observations for each ID pair
meanweight = mean(weightedEL.groom$weight[weightedEL.groom$weight>0]) #compute nonzero mean weight
weightedEL.groom$weight <- weightedEL.groom$weight/meanweight
el = weightedEL.groom[,c("givingID","receivingID",'weight'),];
adjMat = dils::AdjacencyFromEdgelist(el)# create adjacency matrix based on edge list.
mat = adjMat[["adjacency"]]; rownames(mat) = adjMat[["nodelist"]]; colnames(mat) = adjMat[["nodelist"]]
net<-as.network.matrix(mat,loops=FALSE,directed=T)
isPost = c(0,1); h=1
for (h in 1:length(isPost)){ #pre- and post-hurricane
rscans = randscansY[which(randscansY$isPost==isPost[h]),] #subselect scans of group G, year Y and hurricane status H
numscans = as.data.frame(table(as.character(rscans$focalID))); names(numscans) =c("id","freq")
#Find all unique IDs
unqIDs = unique(as.character(rscans$focalID))
# Output the Master Edgelist of all possible pairs given the unique IDs.
masterEL = calcMasterEL_groom(unqIDs)
# Output weighted edgelist from the Master Edgelist for grooming network
weightedEL.groom = calcEdgeList_groom(rscans,masterEL)
weightedEL.groom = weightedEL.groom[,c("givingID","receivingID","count")]
weightedEL.groom$conc = paste(weightedEL.groom$givingID, weightedEL.groom$receivingID, sep=".")
weightedEL.groom$numscans <- (numscans$freq[match(weightedEL.groom$givingID, numscans$id)] + numscans$freq[match(weightedEL.groom$receivingID, numscans$id)])/2
weightedEL.groom$weight <- round(weightedEL.groom$count / weightedEL.groom$numscans, 5) #add weight information by dividing by avg #observations for each ID pair
meanweight = mean(weightedEL.groom$weight[weightedEL.groom$weight>0]) #compute nonzero mean weight
weightedEL.groom$weight <- weightedEL.groom$weight/meanweight
el = weightedEL.groom[,c("givingID","receivingID",'weight'),];
adjMat = dils::AdjacencyFromEdgelist(el)# create adjacency matrix based on edge list.
mat = adjMat[["adjacency"]]; rownames(mat) = adjMat[["nodelist"]]; colnames(mat) = adjMat[["nodelist"]]
net<-as.network.matrix(mat,loops=FALSE,directed=T)
#set kinship as edge attribute
KC      <- NULL; for(i in 1:length(el[,1])){
KC[i] <-  ped[which(rownames(ped)==as.character(el$receivingID[i])) , which(colnames(ped)==as.character(el$givingID[i]))]
}
set.network.attribute(net, "kinship",KC)
#set proximity as edge attribute
options(warn = -1) #set options to ignore all warnings
weightedEL.prox = calcEdgeList(rscans,masterEL) #Get counts for proximity network
weightedEL.prox$numscans <- (numscans$freq[match(weightedEL.prox$givingID, numscans$id)] + numscans$freq[match(weightedEL.prox$receivingID, numscans$id)])/2
weightedEL.prox$weight <- round(weightedEL.prox$count / weightedEL.prox$numscans, 5) #add weight information by dividing by avg #observations for each ID pair
meanweight = mean(weightedEL.prox$weight[weightedEL.prox$weight>0]) #compute nonzero mean weight
weightedEL.prox$weight <- weightedEL.prox$weight/meanweight
el.prox = weightedEL.prox[,c("givingID","receivingID",'weight'),];
prox <-NULL; for(i in 1:nrow(el.prox)){
prox[i]<- el.prox$weight[which(el.prox$givingID==el$givingID[i] & el.prox$receivingID==el$receivingID[i])]
}
set.network.attribute(net, "prox",prox)
if(isPost[h]==0){net.pre=net}
if(isPost[h]==1){net.post=net}
}
isPost = c(0,1); h=1
for (h in 1:length(isPost)){ #pre- and post-hurricane
rscans = randscansY[which(randscansY$isPost==isPost[h]),] #subselect scans of group G, year Y and hurricane status H
numscans = as.data.frame(table(as.character(rscans$focalID))); names(numscans) =c("id","freq")
#Find all unique IDs
unqIDs = unique(as.character(rscans$focalID))
# Output the Master Edgelist of all possible pairs given the unique IDs.
masterEL = calcMasterEL_groom(unqIDs)
# Output weighted edgelist from the Master Edgelist for grooming network
weightedEL.groom = calcEdgeList_groom(rscans,masterEL)
weightedEL.groom = weightedEL.groom[,c("givingID","receivingID","count")]
weightedEL.groom$conc = paste(weightedEL.groom$givingID, weightedEL.groom$receivingID, sep=".")
weightedEL.groom$numscans <- (numscans$freq[match(weightedEL.groom$givingID, numscans$id)] + numscans$freq[match(weightedEL.groom$receivingID, numscans$id)])/2
weightedEL.groom$weight <- round(weightedEL.groom$count / weightedEL.groom$numscans, 5) #add weight information by dividing by avg #observations for each ID pair
meanweight = mean(weightedEL.groom$weight[weightedEL.groom$weight>0]) #compute nonzero mean weight
weightedEL.groom$weight <- weightedEL.groom$weight/meanweight
el = weightedEL.groom[,c("givingID","receivingID",'weight'),];
adjMat = dils::AdjacencyFromEdgelist(el)# create adjacency matrix based on edge list.
mat = adjMat[["adjacency"]]; rownames(mat) = adjMat[["nodelist"]]; colnames(mat) = adjMat[["nodelist"]]
net<-as.network.matrix(mat,loops=FALSE,directed=T)
#set kinship as edge attribute
# KC      <- NULL; for(i in 1:length(el[,1])){
#   KC[i] <-  ped[which(rownames(ped)==as.character(el$receivingID[i])) , which(colnames(ped)==as.character(el$givingID[i]))]
# }
# set.network.attribute(net, "kinship",KC)
#set proximity as edge attribute
options(warn = -1) #set options to ignore all warnings
weightedEL.prox = calcEdgeList(rscans,masterEL) #Get counts for proximity network
weightedEL.prox$numscans <- (numscans$freq[match(weightedEL.prox$givingID, numscans$id)] + numscans$freq[match(weightedEL.prox$receivingID, numscans$id)])/2
weightedEL.prox$weight <- round(weightedEL.prox$count / weightedEL.prox$numscans, 5) #add weight information by dividing by avg #observations for each ID pair
meanweight = mean(weightedEL.prox$weight[weightedEL.prox$weight>0]) #compute nonzero mean weight
weightedEL.prox$weight <- weightedEL.prox$weight/meanweight
el.prox = weightedEL.prox[,c("givingID","receivingID",'weight'),];
prox <-NULL; for(i in 1:nrow(el.prox)){
prox[i]<- el.prox$weight[which(el.prox$givingID==el$givingID[i] & el.prox$receivingID==el$receivingID[i])]
}
set.network.attribute(net, "prox",prox)
if(isPost[h]==0){net.pre=net}
if(isPost[h]==1){net.post=net}
}
#combine pre- and post networks
prePostList = list(net.pre, net.post)
prePostNet <- networkDynamic(network.list=prePostList)#creating a "dynamic network" structure for temporal analyses
View(prePostList)
#plot networks
par(mfrow = c(2,2), oma=c(1,1,1,1), mar=c(4,1,1,1))
plot(network.extract(prePostNet, at = 0), main = "Time 1",
displaylabels = T, label.cex = 0.6, vertex.cex = 2, pad = 0.5)
plot(network.extract(prePostNet, at = 1), main = "Time2",
displaylabels = T, label.cex = 0.6, vertex.cex = 2, pad = 0.5)

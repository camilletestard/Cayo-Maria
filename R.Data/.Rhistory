A.Weightdeg<-degree(data) #
A.Weightdeg
A.Weight.IN.deg <-degree(data, gmode="digraph", diag=FALSE, tmaxdev=FALSE, cmode="indegree", rescale=FALSE, ignore.eval=FALSE)
A.Weight.OUT.deg <-degree(data, gmode="digraph", diag=FALSE, tmaxdev=FALSE, cmode="outdegree", rescale=FALSE, ignore.eval=FALSE)
A.Weight.IN.deg
A.Weight.OUT.deg
A.Bindeg<-apply(data,2,function(a)sum(a>0))
A.Boutdeg<-apply(data,1,function(a)sum(a>0))
A.Clust<-transitivity(am.g,"local")
#clustering coeff (local). How close are the neighbors of a graph to be a clique (a complete graph). Are your friends also friends between each other?
A.Clust<-transitivity(am.g,"local") #from igraph package
#Weighted betweenness.The number of shortest paths that pass through the vertex.
A.Weight.between<-betweenness(data,gmode="graph",ignore.eval=F)
#Weighted eigenvector centrality
A.Weight.eig.cen<-evcent(data,gmode="graph",ignore.eval=F)
A.Weight.eig.cen<-evcent(data,gmode="graph",ignore.eval=F) #from 'sna' package
networkMeasures<-data.frame(cbind(A.Weightdeg,A.Weight.IN.deg,A.Weight.OUT.deg, A.Bindeg, A.Boutdeg, A.Clust,A.Weight.between,A.Weight.eig.cen))
write.table(networkMeasures, file = "A.2016HH_Centrality_Output.csv", sep = ",", col.names=T, row.names=T)
View(networkMeasures)
View(networkMeasures)
vcount(am.g)
vcount(am.g)^2.3
vcount(am.g)^2.8
V(am.g)
V(am.g)$label.cex <- 0.5
#increase space between nodes if overlapping
#fruchterman reingold layout
l <- layout.fruchterman.reingold(am.g,niter=500,area=vcount(am.g)^2.3,repulserad=vcount(am.g)^2.8)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.5
View(l)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name*2, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name*0.5, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name*0.5, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
V(am.g)$name
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.5,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
#fruchterman reingold layout
l <- layout.fruchterman.reingold(am.g,niter=500,area=vcount(am.g)^3.3,repulserad=vcount(am.g)^3.8)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.5
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
vcount(am.g)
vcount(am.g)^3.3
l <- layout.fruchterman.reingold(am.g,niter=500,area=vcount(am.g)^2,repulserad=vcount(am.g)^2.3)
#changes size of labels of vertices
V(am.g)$label.cex <- 0.5
#plot graph
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*2,edge.arrow.size = 0.5)
plot.igraph(am.g,layout=l, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.label=V(am.g)$name, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
plot.igraph(am.g,layout=l, vertex.color="CYAN1", vertex.size=7,edge.color="grey20",
edge.width=E(am.g)$weight*0.2,edge.arrow.size = 0.2)
#spring embedded layout
s <- layout.spring(am.g, spring.length=1000,area=vcount(am.g)^2.3,repulserad=vcount(am.g)^2.8)
plot.igraph(am.g,layout=l, vertex.label=NA, vertex.color="orange", vertex.size=6,edge.color="grey20",edge.width=E(am.g)$weight*0.01)
plot.igraph(am.g,layout=l, vertex.label=NA, vertex.color="orange", vertex.size=6,edge.color="grey20",edge.width=E(am.g)$weight*0.01, edge.arrow.size = 0.2)
View(networkMeasures)
A.Weight.IN.deg
A.Weightdeg
A.Weight.OUT.deg
A.Weight.between
A.Weight.between
A.Weight.between<-betweenness(data,gmode="digraph",ignore.eval=F)
A.Weight.between
A.Weight.closenessn<-closeness(data,gmode="digraph",ignore.eval=F) #from 'sna' package
A.Weight.closeness<-closeness(data,gmode="digraph",ignore.eval=F) #from 'sna' package
A.Weight.closeness
A.Weight.closeness<-closeness(data,gmode="digraph",ignore.eval=F)
A.Weight.closeness
closeness(data,gmode="digraph",ignore.eval=F)
evcent(data,gmode="graph",ignore.eval=F)
closeness(data,gmode="graph",ignore.eval=F)
A.Weight.eig.cen<-evcent(data,gmode="digraph",ignore.eval=F)
A.Weight.eig.cen
A.Weight.eig.cen<-evcent(data,gmode="graph",ignore.eval=F)
A.Weight.eig.cen
install.packages("Perc", dependencies = TRUE)
install.packages("fitdistrplus", dependencies = TRUE)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/ModelEffects.RData")
hist(NotAloneEffects$isPost)
hist(SocialEffects$isPost)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/ModelEffects.RData")
hist(NotAloneEffects$isPost)
hist(SocialEffects$isPost)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/ModelEffects.RData")
install.packages("gdata",dependencies = T)
keep(c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects"))
gdata::keep(c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects"))
c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects")
rm(list=setdiff(ls(),c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects")))
hist(NotAloneEffects$isPost)
hist(NotAloneEffects$isPost,add =F)
hist(SocialEffects$isPost,add =T)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/ModelEffects.RData")
rm(list=setdiff(ls(),c("NotAlone.G.Effects","NotAlone.KK.Effects","NotAlone.V.Effects","NotAlone.S.Effects",
"NotAlone.Q.Effects","NotAlone.PM.Effects","NotAloneEffects","Social.G.Effects", "Social.KK.Effects",
"Social.V.Effects","Social.S.Effects","Social.Q.Effects","Social.PM.Effects","SocialEffects")))
#1. Show the distribution of the Hurricane effect on P(Acc) and P(Social) considering all data:
hist(NotAloneEffects$isPost,add =F)
hist(NotAloneEffects$isPost,add =F)
hist(SocialEffects$isPost,add =TRUE)
library(ggplot2)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/AllStats3.RData")
AllStats[["KK.2013.1"]]= NULL
groupyear = c("V.2015","V.2016","V.2017","KK.2015","KK.2017")
gy=1
paste(c("GlobalNetworkMetrics",groupyear[gy],"pdf"),sep = ".")
paste("GlobalNetworkMetrics",groupyear[gy],"pdf",sep = ".")
groupyear = c("V.2015","V.2016","V.2017","KK.2015","KK.2017")
for (gy in 1:length(groupyear)){ #For each group
name.0 = paste(groupyear[gy],".0",sep="")
data.0 = AllStats[[name.0]]; data.0$isPost = 0
name.1 = paste(groupyear[gy],".1",sep="")
data.1 = AllStats[[name.1]]; data.1$isPost = 1
data= rbind(data.0, data.1)
pdf(file= paste("GlobalNetworkMetrics",groupyear[gy],"pdf",sep = "."), width=5, height=5, onefile = T) #width and height of the graphics region in inches. One file if true = multiple graphs in one file
#hist(data.0$dens); hist(data.1$dens)
{density <- ggplot(data, aes(x= as.factor(isPost), y=dens, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Density of social network pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Density of social network")}
#ylim(0,0.4)
# print(density)
# readline(prompt = "pause ")
#hist(data.0$gini); hist(data.1$gini)
{equity <- ggplot(data, aes(x= as.factor(isPost), y=gini, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Equity of social network pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Gini coeff relative to baseline")}
#ylim(0,0.9)
# print(equity)
# readline(prompt = "pause ")
}
library(ggplot2)
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/AllStats4.RData")
##########################################################
#Sex partner preference
##########################################################
groupyear = c("V.2015","V.2016","V.2017","KK.2015","KK.2017")
for (gy in 1:length(groupyear)){ #For each group
name.0 = paste(groupyear[gy],".0",sep="")
data.0 = AllStats[[name.0]]; data.0$isPost = 0
name.1 = paste(groupyear[gy],".1",sep="")
data.1 = AllStats[[name.1]]; data.1$isPost = 1
data= rbind(data.0, data.1)
pdf(file= paste("SexPartnerPref",groupyear[gy],"pdf",sep = "."), width=5, height=5, onefile = T)
{FFpair <- ggplot(data, aes(x= as.factor(isPost), y=eo.FF, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("FF Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. FF")}
#ylim(0,0.4)
# print(FFpair)
# readline(prompt = "pause ")
{MMpair <- ggplot(data, aes(x= as.factor(isPost), y=eo.MM, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("MM Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. MM")}
#ylim(0,0.4)
# print(MMpair)
# readline(prompt = "pause ")
{crosspair <- ggplot(data, aes(x= as.factor(isPost), y=eo.cross, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Cross Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. Cross")}
#ylim(0,0.4)
# print(crosspair)
# readline(prompt = "pause ")
}
##########################################################
#Kinship preference
##########################################################
for (gy in 1:length(groupyear)){ #For each group
name.0 = paste(groupyear[gy],".0",sep="")
data.0 = AllStats[[name.0]]; data.0$isPost = 0
name.1 = paste(groupyear[gy],".1",sep="")
data.1 = AllStats[[name.1]]; data.1$isPost = 1
data= rbind(data.0, data.1)
pdf(file= paste("KinshipPartnerPref",groupyear[gy],"pdf",sep = "."), width=5, height=5, onefile = T)
CKpair <- ggplot(data, aes(x= as.factor(isPost), y=eo.ck, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Close Kin Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. Close Kin")
#ylim(0,0.4)
print(CKpair)
readline(prompt = "pause ")
DKpair <- ggplot(data, aes(x= as.factor(isPost), y=eo.dk, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Distant Kin Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. Distant Kin")
#ylim(0,0.4)
print(DKpair)
readline(prompt = "pause ")
Upair <- ggplot(data, aes(x= as.factor(isPost), y=eo.u, fill=as.factor(isPost) ))+
geom_boxplot()+
geom_jitter(position = position_jitter(0.2), alpha = 0.5)+
ggtitle(paste("Unrelated Pair preference pre- vs. post- hurricane ",groupyear[gy],sep=""))+
labs(fill = "Hurricane Status",x="Hurricane Status",y="Obs./Exp. Unrelated")
#ylim(0,0.4)
print(Upair)
readline(prompt = "pause ")
}
library(lme4)# Generalized Linear Mixed Models
library(lmerTest)
library(performance)
#library(sjPlot)
#library(glmmTMB)# Generalized Linear Mixed Models, other package
#library(MCMCglmm)# Generalized Linear Mixed Models, other package
#library(bbmle)#Tools for General Maximum Likelihood Estimation
#library(DHARMa) #residual diagnostic fr hierarchical (multi-level/mixed) regression models
#Load data
load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/Social_Network_Analysis/SocialCapital.dSocialRates.RData")
#Format data correclty
SocialCapital.ALL$sex = as.factor(SocialCapital.ALL$sex); SocialCapital.ALL$age = as.numeric(SocialCapital.ALL$age); SocialCapital.ALL$group = as.factor(SocialCapital.ALL$group)
data = SocialCapital.ALL[-which(is.na(SocialCapital.ALL$dpAcc)),] #remove NA
#check distribution of independent variables
hist(data$dpAcc); hist(data$dpSocial, add=T)
#Scale parameters:
data[,c("age","GroomIN","GroomOUT","AggIN","AggOUT","vig.ra","sdb.ra")] <- scale(data[,c("age","GroomIN","GroomOUT","AggIN","AggOUT","vig.ra","sdb.ra")])
## Model Social Capital effect on change in sociliaty rates
#Modelling change in p(Acc)
dpAcc1 <- lmer(dpAcc~ sex + age + group + rank + (1|id) + (1|year), data = data)
summary(dpAcc1)
tab_model(dpAcc1)
install.packages("tsna",dependencies = T)
install.packages("ndtv",dependencies = T)
######## Modelling Logistic Regressions on A2
library(lme4)# Generalized Linear Mixed Models
library(glmmTMB)
library(bbmle)#Tools for General Maximum Likelihood Estimation
library(DHARMa) #residual diagnostic fr hierarchical (multi-level/mixed) regression models
library(jtools)
library(data.table)
library(ggpubr)
library(dplyr)
library(fitdistrplus)
library(lmtest)
#Load AllScans file
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/")
source("cleaned_code/Functions/CalcSubsampledScans.R")
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2015.txt")
#Run glmm on social rates
iter = 500;  start_time <- Sys.time()
NotAloneEffects = data.frame(); SocialEffects = data.frame();
#Load previous
# load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data/ModelEffectsFinal2015.RData")
i=1
ExSubScans = calcRandomScans(allScans)
#Scale parameters
ExSubScans[,"age"] <- scale(ExSubScans[,"age"]) #helps avoid convergence issues: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
ExSubScans$percentrank <- ExSubScans$percentrank/100
ExSubScans$year <- as.factor(ExSubScans$year); ExSubScans$isPost <- as.factor(ExSubScans$isPost)
ExSubScans$isProx <- as.factor(ExSubScans$isProx); ExSubScans$isSocial <- as.factor(ExSubScans$isSocial)
isSocial <- glmer(isSocial~ isPost+ sex + age + percentrank + group + timeBlock + Q +(1|focalID) + (1|year), data = ExSubScans, family = binomial)
print(summary(isSocial))
isSocial <- glmer(isSocial~ isPost+ sex + age + percentrank + group + timeBlock + Q +(1|focalID) + (1|year), data = ExSubScans, family = binomial)
print(summary(isSocial))
isSocial <- glmer(isSocial~ isPost +(1|focalID) + (1|year), data = ExSubScans, family = binomial)
print(summary(isSocial))
######## Modelling Logistic Regressions on A2
library(lme4)# Generalized Linear Mixed Models
library(glmmTMB)
library(bbmle)#Tools for General Maximum Likelihood Estimation
library(DHARMa) #residual diagnostic fr hierarchical (multi-level/mixed) regression models
library(jtools)
library(data.table)
library(ggpubr)
library(dplyr)
library(fitdistrplus)
library(lmtest)
#Load AllScans file
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/")
source("cleaned_code/Functions/CalcSubsampledScans.R")
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2015.txt")
#Run glmm on social rates
iter = 500;  start_time <- Sys.time()
NotAloneEffects = data.frame(); SocialEffects = data.frame();
#Load previous
# load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data/ModelEffectsFinal2015.RData")
i=1
for (i in 1:iter) {
print(paste("%%%%%%%%%%%%%%%%%% iter",i, "%%%%%%%%%%%%%%%%%%"))
ExSubScans = calcRandomScans(allScans)
#Scale parameters
ExSubScans[,"age"] <- scale(ExSubScans[,"age"]) #helps avoid convergence issues: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
ExSubScans$percentrank <- ExSubScans$percentrank/100
ExSubScans$year <- as.factor(ExSubScans$year); ExSubScans$isPost <- as.factor(ExSubScans$isPost)
ExSubScans$isProx <- as.factor(ExSubScans$isProx); ExSubScans$isSocial <- as.factor(ExSubScans$isSocial)
# t=table(as.character(ExSubScans$focalID),ExSubScans$isPost)
# write.csv(t,"C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Noah_files_check/onlyID_categ.csv")
##########################################################
##BASE MODEL
##########################################################
isNotAlone <- glmer(isProx~ isPost + (1|focalID) + (1|year), data = ExSubScans, family = binomial) #Note: might want to try MCMCglmm?
print(summary(isNotAlone))
# test <- glmer(isProx~ isPost + sex + age + percentrank + group + (1|focalID) + (1|year), data = ExSubScans, family = binomial); summary(test)
#Note: because we balance ID, sex, rank etc. we do not actually need to control for these factors...
# simres1 <- simulateResiduals(isNotAlone, n = 1000)
# testResiduals(simres1)
# performance::check_model(isNotAlone) #this allow to check for multicollinearity, homogeneity of variance, nromality of random effects, normality of residuals and outliers
NotAloneEffects[i,c("(Intercept)","isPost","sexM","age","rank","groupV")] <- getME(isNotAlone, "beta")
NotAloneEffects[i,c("(focalID)","(year)")] <- getME(isNotAlone, "theta")
isSocial <- glmer(isSocial~ isPost +(1|focalID) + (1|year), data = ExSubScans, family = binomial)
print(summary(isSocial))
# simres2 <- simulateResiduals(isSocial, n = 1000)
# testResiduals(simres2)
# performance::check_model(isSocial)
SocialEffects[i,c("(Intercept)","isPost","sexM","age","rank","groupV")] <- getME(isSocial, "beta")
SocialEffects[i,c("(focalID)","(year)")] <- getME(isSocial, "theta")
save.image("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data/ModelEffectsFinal2015.RData")
}
end_time <- Sys.time()
end_time - start_time
######## Modelling Logistic Regressions on A2
library(lme4)# Generalized Linear Mixed Models
library(glmmTMB)
library(bbmle)#Tools for General Maximum Likelihood Estimation
library(DHARMa) #residual diagnostic fr hierarchical (multi-level/mixed) regression models
library(jtools)
library(data.table)
library(ggpubr)
library(dplyr)
library(fitdistrplus)
library(lmtest)
#Load AllScans file
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/")
source("cleaned_code/Functions/CalcSubsampledScans.R")
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2015.txt")
#Run glmm on social rates
iter = 500;  start_time <- Sys.time()
NotAloneEffects = data.frame(); SocialEffects = data.frame();
#Load previous
# load("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data/ModelEffectsFinal2015.RData")
i=1
for (i in 1:iter) {
print(paste("%%%%%%%%%%%%%%%%%% iter",i, "%%%%%%%%%%%%%%%%%%"))
ExSubScans = calcRandomScans(allScans)
#Scale parameters
ExSubScans[,"age"] <- scale(ExSubScans[,"age"]) #helps avoid convergence issues: https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
ExSubScans$percentrank <- ExSubScans$percentrank/100
ExSubScans$year <- as.factor(ExSubScans$year); ExSubScans$isPost <- as.factor(ExSubScans$isPost)
ExSubScans$isProx <- as.factor(ExSubScans$isProx); ExSubScans$isSocial <- as.factor(ExSubScans$isSocial)
# t=table(as.character(ExSubScans$focalID),ExSubScans$isPost)
# write.csv(t,"C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Noah_files_check/onlyID_categ.csv")
##########################################################
##BASE MODEL
##########################################################
isNotAlone <- glmer(isProx~ isPost + sex + age + percentrank + group + (1|focalID) + (1|year), data = ExSubScans, family = binomial) #Note: might want to try MCMCglmm?
print(summary(isNotAlone))
# test <- glmer(isProx~ isPost + sex + age + percentrank + group + (1|focalID) + (1|year), data = ExSubScans, family = binomial); summary(test)
#Note: because we balance ID, sex, rank etc. we do not actually need to control for these factors...
# simres1 <- simulateResiduals(isNotAlone, n = 1000)
# testResiduals(simres1)
# performance::check_model(isNotAlone) #this allow to check for multicollinearity, homogeneity of variance, nromality of random effects, normality of residuals and outliers
NotAloneEffects[i,c("(Intercept)","isPost","sexM","age","rank","groupV")] <- getME(isNotAlone, "beta")
NotAloneEffects[i,c("(focalID)","(year)")] <- getME(isNotAlone, "theta")
isSocial <- glmer(isSocial~ isPost + sex + age + percentrank + group +(1|focalID) + (1|year), data = ExSubScans, family = binomial)
print(summary(isSocial))
# simres2 <- simulateResiduals(isSocial, n = 1000)
# testResiduals(simres2)
# performance::check_model(isSocial)
SocialEffects[i,c("(Intercept)","isPost","sexM","age","rank","groupV")] <- getME(isSocial, "beta")
SocialEffects[i,c("(focalID)","(year)")] <- getME(isSocial, "theta")
save.image("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data/ModelEffectsFinal2015.RData")
}
end_time <- Sys.time()
end_time - start_time
#load local functions
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/cleaned_code")
source("Functions/functions_SocialSupport.R")
#Load scan data, population and dominance info
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
dominance_info =read.table("Behavioral_Data/Database Complete/Data All Raw/DOMINANCE.txt",header = T)
#Set parameters
network_action = "groom"
network_mode = "directed"
network_weighted = "weighted" # "unweighted"
#For each group, each year separately:
group = c("V","V","V","KK","KK")
years = c(2015,2016,2017,2015, 2017)
groupyears =c("V2015","V2016","V2017","KK2015","KK2017"); gy=1
allEL.Focal = list(); ID.list=list()
for (gy in 1:length(groupyears)){ #For each group
#####################################################
#FOR GROOMING
#####################################################
#Load data
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Behavioral_Data/Data All Cleaned")
meta_data=read.csv(paste("Group",groupyears[gy],"_GroupByYear.txt",sep=""))
data = data=read.csv(paste("Group",groupyears[gy],"_GroomingEvents.txt",sep=""))
#create date of observation information:
data$date = mdy(gsub(".","-",substr(data$observation.session,1,8),fixed=T))
data$semester = semester(data$date)
data$quarter = quarter(data$date)
#Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = as.character(unique(meta_data$id))
masterEL = calcMasterEL_groom(unqIDs)
# 4. Output weighted edgelist from the Master Edgelist.
data$conc <- paste(data[,1],data[,2],sep="."); weightedEL = masterEL
# Transform edgelist name for later coding
weightedEL$alter = weightedEL$givingID; weightedEL$ego = weightedEL$receivingID
weightedEL$givingID <- NULL; weightedEL$receivingID <-NULL
weightedEL = weightedEL[,c("alter","ego","conc","count")]
#count the duration of pair grooming
count = data.frame(table(data$conc))
for (i in 1:nrow(weightedEL)){
weightedEL$count[i] = sum(data$duration[which(data$conc == weightedEL$conc[i])]) #find the time spent grooming for each pair
}
weightedEL$count[which(is.na(weightedEL$count))] = 0
weightedEL$hrs <- (meta_data$hrs.focalfollowed[match(weightedEL$alter, meta_data$id)] +
meta_data$hrs.focalfollowed[match(weightedEL$ego, meta_data$id)])/2
weightedEL$weight <- round(weightedEL$count / weightedEL$hrs, 5) #add weight information by dividing by the #scans for each individual
#check: which(weightedEL$weight>0); which(weightedEL$count>0)
meanWeight = mean(weightedEL$weight[which(weightedEL$weight!=0)])
weightedEL$std.weight <- weightedEL$weight/meanWeight #Normalize weights
#Save weighted EL
allEL.Focal[[gy]]=weightedEL
weightedEL$count <- NULL;weightedEL$conc <- NULL; weightedEL$hrs <-NULL #delete those calumn variables
ID.list[[gy]]=as.character(meta_data$id)
}
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data")
save(allEL.Focal,ID.list, file=paste(network_weighted,"allEL.Focal.RData",sep="."))
#load local functions
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/cleaned_code")
source("Functions/functions_GlobalNetworkMetrics.R")
#Load scan data, population and dominance info
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
dominance_info =read.table("Behavioral_Data/Database Complete/Data All Raw/DOMINANCE.txt",header = T)
#Set parameters
network_action = "groom"
network_mode = "directed"
network_weighted = "weighted" # "unweighted"
#For each group, each year separately:
group = c("V","V","V","KK","KK")
years = c(2015,2016,2017,2015, 2017)
groupyears =c("V2015","V2016","V2017","KK2015","KK2017"); gy=1
allEL.Focal = list(); ID.list=list()
for (gy in 1:length(groupyears)){ #For each group
#####################################################
#FOR GROOMING
#####################################################
#Load data
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Behavioral_Data/Data All Cleaned")
meta_data=read.csv(paste("Group",groupyears[gy],"_GroupByYear.txt",sep=""))
data = data=read.csv(paste("Group",groupyears[gy],"_GroomingEvents.txt",sep=""))
# #create date of observation information:
# data$date = mdy(gsub(".","-",substr(data$observation.session,1,8),fixed=T))
# data$semester = semester(data$date)
# data$quarter = quarter(data$date)
#Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = as.character(unique(meta_data$id))
masterEL = calcMasterEL_groom(unqIDs)
# 4. Output weighted edgelist from the Master Edgelist.
data$conc <- paste(data[,1],data[,2],sep="."); weightedEL = masterEL
# Transform edgelist name for later coding
weightedEL$alter = weightedEL$givingID; weightedEL$ego = weightedEL$receivingID
weightedEL$givingID <- NULL; weightedEL$receivingID <-NULL
weightedEL = weightedEL[,c("alter","ego","conc","count")]
#count the duration of pair grooming
count = data.frame(table(data$conc))
for (i in 1:nrow(weightedEL)){
weightedEL$count[i] = sum(data$constrained_duration[which(data$conc == weightedEL$conc[i])]) #find the time spent grooming for each pair
}
weightedEL$count[which(is.na(weightedEL$count))] = 0
weightedEL$hrs <- (meta_data$hrs.focalfollowed[match(weightedEL$alter, meta_data$id)] +
meta_data$hrs.focalfollowed[match(weightedEL$ego, meta_data$id)])/2
weightedEL$weight <- round(weightedEL$count / weightedEL$hrs, 5) #add weight information by dividing by the #scans for each individual
#check: which(weightedEL$weight>0); which(weightedEL$count>0)
meanWeight = mean(weightedEL$weight[which(weightedEL$weight!=0)])
weightedEL$std.weight <- weightedEL$weight/meanWeight #Normalize weights
#Save weighted EL
allEL.Focal[[gy]]=weightedEL
weightedEL$count <- NULL;weightedEL$conc <- NULL; weightedEL$hrs <-NULL #delete those calumn variables
ID.list[[gy]]=as.character(meta_data$id)
}
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data")
save(allEL.Focal,ID.list, file=paste(network_weighted,"allEL.Focal.RData",sep="."))

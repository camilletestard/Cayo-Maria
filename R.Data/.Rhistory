xlab("Pre-disaster Grooming")+
ylab("Strength of bond to deceased IDs pre-disaster (standardized)")+
facet_grid(~sex)
ggplot(data, aes(x=dead.all, y=dpSocial))+
geom_point(alpha = 0.25, size = 3) +
geom_smooth(method=lm, color=rgb(1,0,0))+
ggtitle("Post-disaster change in grooming as a function of pre-disaster strength of bond to deceased IDs")+
xlab("Pre-disaster Grooming")+
ylab("Strength of bond to deceased IDs pre-disaster (standardized)")
cor.test(data$dpSocial,data$dead.all)
#load local functions
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/cleaned_code")
source("Functions/functions_SocialSupport.R")
#Load scan data, population and dominance info
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
dominance_info =read.table("Behavioral_Data/Database Complete/Data All Raw/DOMINANCE.txt",header = T)
#Set parameters
network_action = "groom"
network_mode = "directed"
network_weighted = "weighted" # "unweighted"
#For each group, each year separately:
group = c("V","V","V","KK","KK")
years = c(2015,2016,2017,2015, 2017)
groupyears =c("V2015","V2016","V2017","KK2015","KK2017"); gy=1
allEL.Focal = list(); ID.list=list()
for (gy in 1:length(groupyears)){ #For each group
#####################################################
#FOR GROOMING
#####################################################
#Load data
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Behavioral_Data/Data All Cleaned")
meta_data=read.csv(paste("Group",groupyears[gy],"_GroupByYear.txt",sep=""))
data = data=read.csv(paste("Group",groupyears[gy],"_GroomingEvents.txt",sep=""))
#create date of observation information:
data$date = mdy(gsub(".","-",substr(data$observation.session,1,8),fixed=T))
data$semester = semester(data$date)
data$quarter = quarter(data$date)
#Output the Master Edgelist of all possible pairs given the unique IDs.
unqIDs = as.character(unique(meta_data$id))
masterEL = calcMasterEL_groom(unqIDs)
# 4. Output weighted edgelist from the Master Edgelist.
data$conc <- paste(data[,1],data[,2],sep="."); weightedEL = masterEL
# Transform edgelist name for later coding
weightedEL$alter = weightedEL$givingID; weightedEL$ego = weightedEL$receivingID
weightedEL$givingID <- NULL; weightedEL$receivingID <-NULL
weightedEL = weightedEL[,c("alter","ego","conc","count")]
#count the duration of pair grooming
count = data.frame(table(data$conc))
for (i in 1:nrow(weightedEL)){
weightedEL$count[i] = sum(data$duration[which(data$conc == weightedEL$conc[i])]) #find the time spent grooming for each pair
}
weightedEL$count[which(is.na(weightedEL$count))] = 0
weightedEL$hrs <- (meta_data$hrs.focalfollowed[match(weightedEL$alter, meta_data$id)] +
meta_data$hrs.focalfollowed[match(weightedEL$ego, meta_data$id)])/2
weightedEL$weight <- round(weightedEL$count / weightedEL$hrs, 5) #add weight information by dividing by the #scans for each individual
#check: which(weightedEL$weight>0); which(weightedEL$count>0)
meanWeight = mean(weightedEL$weight[which(weightedEL$weight!=0)])
weightedEL$std.weight <- weightedEL$weight/meanWeight #Normalize weights
#Save weighted EL
allEL.Focal[[gy]]=weightedEL
weightedEL$count <- NULL;weightedEL$conc <- NULL; weightedEL$hrs <-NULL #delete those calumn variables
ID.list[[gy]]=as.character(meta_data$id)
}
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data")
save(allEL.Focal,ID.list, file=paste(network_weighted,"allEL.Focal.RData",sep="."))
#Find whether individuals who lost partners are the most sensitive to the post-hurricane effect.
#Load data
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
Demographics = read.csv("Behavioral_Data/CPRCdemographicfile_acquired_03.2020.csv"); names(Demographics)[1]="id"
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2019.txt");
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data")
load("weighted.allEL.Focal.RData")
#Find study population
study.pop = unique(unlist(ID.list)) #all individuals in group by year files from pre-hurricane years in group V and KK
#Format date, letters etc.
Demographics$Sex = toupper(Demographics$Sex);
Demographics$DateTransfer = NULL; Demographics$EventDate = NULL; Demographics$Last.Event = NULL;
Demographics$DOD <- mdy(as.character(Demographics$DOD))
Demographics$YOD <- year(as.character(Demographics$DOD))
Demographics$MOD <- month(as.character(Demographics$DOD))
Demographics$DOB <- mdy(as.character(Demographics$DOB))
Demographics$AgeAtDeath = Demographics$YOD - Demographics$BirthSeason
Demographics$AgeAtDeath[Demographics$AgeAtDeath <0]=0 #Note: there are discrepancies between birth season and death date
# boxplot(Demographics$AgeAtDeath[Demographics$AgeAtDeath >0])
####################################################
# Find who died in V and KK post hurricane
####################################################
Deaths.postHurr =  filter(Demographics,Demographics$DOD >= as.Date("2017-09-01") & Demographics$DOD < as.Date("2018-06-01"))#Find deaths before the hurricane
#only onsider V and KK deaths
Deaths.study.pop = filter(Deaths.postHurr, Deaths.postHurr$LastGroup=="V"|Deaths.postHurr$LastGroup=="KK")
#Add old age factor:
Deaths.study.pop$isOld=0; Deaths.study.pop$isOld[Deaths.study.pop$AgeAtDeath>17]=1
# Deaths.study.pop = filter(Deaths.study.pop, Deaths.study.pop$isOld==0)
#only consider deaths of juveniles +
id.matches=match(study.pop,as.character(Deaths.study.pop$id))
Deaths.study.pop = Deaths.study.pop[id.matches[!is.na(id.matches)],]
Deaths.study.ID = as.character(Deaths.study.pop$id)
# table(as.character(Deaths.study.pop$LastGroup))
# table(as.character(Deaths.study.pop$LastGroup), as.character(Deaths.study.pop$Sex))
####################################################
# Find strength of bond from survivors to the deceased
####################################################
group = c("V","V","V","KK","KK")
years = c(2015,2016,2017,2015, 2017)
groupyears =c("V2015","V2016","V2017","KK2015","KK2017"); gy=1;id=1;id.dead=1
strength.to.deceased=data.frame()
for (gy in 1:length(groupyears)){
IDs = ID.list[[gy]]
EL = allEL.Focal[[gy]]
strength = as.data.frame(matrix(0,length(IDs),ncol= 5)); #Initialize ID-level stable partner interaction dataframe
names(strength)=c("id", "group","year","dead.give","dead.get")
for (id in 1:length(IDs)){
for (id.dead in 1:length(Deaths.study.ID)){
if (IDs[id] != Deaths.study.ID[id.dead]){
strength[id,"id"]=IDs[id]; strength[id,"group"]=group[gy]; strength[id,"year"]=years[gy];
strength[id, "dead.give"] = strength$dead.give[id] + sum(EL$weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "std.dead.give"] = strength$std.dead.give[id] + sum(EL$std.weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "dead.get"] = strength$dead.get[id] + sum(EL$weight[which(EL$ego == IDs[id] & EL$alter == Deaths.study.ID[id.dead])])
strength[id, "std.dead.get"] = strength$std.dead.get[id] + sum(EL$std.weight[which(EL$ego == IDs[id] & EL$alter == Deaths.study.ID[id.dead])])
}
}
}
strength.to.deceased=rbind(strength.to.deceased,strength)
}
strength.to.deceased$dead.all=strength.to.deceased$dead.give+strength.to.deceased$dead.get
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data")
save(strength.to.deceased,file="strength.to.deceased.RData")
IDs[id] != Deaths.study.ID[id.dead]
strength[id,"id"]=IDs[id]; strength[id,"group"]=group[gy]; strength[id,"year"]=years[gy];
strength[id, "dead.give"] = strength$dead.give[id] + sum(EL$weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "std.dead.give"] = strength$std.dead.give[id] + sum(EL$std.weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "dead.get"] = strength$dead.get[id] + sum(EL$weight[which(EL$ego == IDs[id] & EL$alter == Deaths.study.ID[id.dead])])
strength[id, "std.dead.get"] = strength$std.dead.get[id] + sum(EL$std.weight[which(EL$ego == IDs[id] & EL$alter == Deaths.study.ID[id.dead])])
strength[id,"id"]=IDs[id]; strength[id,"group"]=group[gy]; strength[id,"year"]=years[gy];
strength[id, "dead.give"] = strength$dead.give[id] + sum(EL$weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "std.dead.give"] = strength$std.dead.give[id] + sum(EL$std.weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
for (gy in 1:length(groupyears)){
IDs = ID.list[[gy]]
EL = allEL.Focal[[gy]]
strength = as.data.frame(matrix(0,length(IDs),ncol= 5)); #Initialize ID-level stable partner interaction dataframe
names(strength)=c("id", "group","year","dead.give","dead.get","std.dead.give","std.dead.get")
for (id in 1:length(IDs)){
for (id.dead in 1:length(Deaths.study.ID)){
if (IDs[id] != Deaths.study.ID[id.dead]){
strength[id,"id"]=IDs[id]; strength[id,"group"]=group[gy]; strength[id,"year"]=years[gy];
strength[id, "dead.give"] = strength$dead.give[id] + sum(EL$weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "std.dead.give"] = strength$std.dead.give[id] + sum(EL$std.weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "dead.get"] = strength$dead.get[id] + sum(EL$weight[which(EL$ego == IDs[id] & EL$alter == Deaths.study.ID[id.dead])])
strength[id, "std.dead.get"] = strength$std.dead.get[id] + sum(EL$std.weight[which(EL$ego == IDs[id] & EL$alter == Deaths.study.ID[id.dead])])
}
}
}
strength.to.deceased=rbind(strength.to.deceased,strength)
}
#Find whether individuals who lost partners are the most sensitive to the post-hurricane effect.
#Load data
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
Demographics = read.csv("Behavioral_Data/CPRCdemographicfile_acquired_03.2020.csv"); names(Demographics)[1]="id"
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2019.txt");
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data")
load("weighted.allEL.Focal.RData")
#Find study population
study.pop = unique(unlist(ID.list)) #all individuals in group by year files from pre-hurricane years in group V and KK
#Format date, letters etc.
Demographics$Sex = toupper(Demographics$Sex);
Demographics$DateTransfer = NULL; Demographics$EventDate = NULL; Demographics$Last.Event = NULL;
Demographics$DOD <- mdy(as.character(Demographics$DOD))
Demographics$YOD <- year(as.character(Demographics$DOD))
Demographics$MOD <- month(as.character(Demographics$DOD))
Demographics$DOB <- mdy(as.character(Demographics$DOB))
Demographics$AgeAtDeath = Demographics$YOD - Demographics$BirthSeason
Demographics$AgeAtDeath[Demographics$AgeAtDeath <0]=0 #Note: there are discrepancies between birth season and death date
# boxplot(Demographics$AgeAtDeath[Demographics$AgeAtDeath >0])
####################################################
# Find who died in V and KK post hurricane
####################################################
Deaths.postHurr =  filter(Demographics,Demographics$DOD >= as.Date("2017-09-01") & Demographics$DOD < as.Date("2018-06-01"))#Find deaths before the hurricane
#only onsider V and KK deaths
Deaths.study.pop = filter(Deaths.postHurr, Deaths.postHurr$LastGroup=="V"|Deaths.postHurr$LastGroup=="KK")
#Add old age factor:
Deaths.study.pop$isOld=0; Deaths.study.pop$isOld[Deaths.study.pop$AgeAtDeath>17]=1
# Deaths.study.pop = filter(Deaths.study.pop, Deaths.study.pop$isOld==0)
#only consider deaths of juveniles +
id.matches=match(study.pop,as.character(Deaths.study.pop$id))
Deaths.study.pop = Deaths.study.pop[id.matches[!is.na(id.matches)],]
Deaths.study.ID = as.character(Deaths.study.pop$id)
# table(as.character(Deaths.study.pop$LastGroup))
# table(as.character(Deaths.study.pop$LastGroup), as.character(Deaths.study.pop$Sex))
####################################################
# Find strength of bond from survivors to the deceased
####################################################
group = c("V","V","V","KK","KK")
years = c(2015,2016,2017,2015, 2017)
groupyears =c("V2015","V2016","V2017","KK2015","KK2017"); gy=1;id=1;id.dead=1
strength.to.deceased=data.frame()
for (gy in 1:length(groupyears)){
IDs = ID.list[[gy]]
EL = allEL.Focal[[gy]]
strength = as.data.frame(matrix(0,length(IDs),ncol= 7)); #Initialize ID-level stable partner interaction dataframe
names(strength)=c("id", "group","year","dead.give","dead.get","std.dead.give","std.dead.get")
for (id in 1:length(IDs)){
for (id.dead in 1:length(Deaths.study.ID)){
if (IDs[id] != Deaths.study.ID[id.dead]){
strength[id,"id"]=IDs[id]; strength[id,"group"]=group[gy]; strength[id,"year"]=years[gy];
strength[id, "dead.give"] = strength$dead.give[id] + sum(EL$weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "std.dead.give"] = strength$std.dead.give[id] + sum(EL$std.weight[which(EL$alter == IDs[id] & EL$ego == Deaths.study.ID[id.dead])])
strength[id, "dead.get"] = strength$dead.get[id] + sum(EL$weight[which(EL$ego == IDs[id] & EL$alter == Deaths.study.ID[id.dead])])
strength[id, "std.dead.get"] = strength$std.dead.get[id] + sum(EL$std.weight[which(EL$ego == IDs[id] & EL$alter == Deaths.study.ID[id.dead])])
}
}
}
strength.to.deceased=rbind(strength.to.deceased,strength)
}
strength.to.deceased$dead.all=strength.to.deceased$dead.give+strength.to.deceased$dead.get
setwd("C:/Users/Camille Testard/Documents/GitHub/Cayo-Maria/R.Data")
save(strength.to.deceased,file="strength.to.deceased.RData")
######## Modelling Logistic Regressions
library(lme4)# Generalized Linear Mixed Models
#library(lmerTest)
library(performance)
library(sjPlot)
library(jtools)
library(ggplot2)
library(dplyr)
#library(glmmTMB)# Generalized Linear Mixed Models, other package
#library(MCMCglmm)# Generalized Linear Mixed Models, other package
#library(bbmle)#Tools for General Maximum Likelihood Estimation
#library(DHARMa) #residual diagnostic fr hierarchical (multi-level/mixed) regression models
#Load data
setwd("C:/Users/Camille Testard/Documents/Github/Cayo-Maria/")
source("cleaned_code/Functions/CalcSubsampledScans.R")
load("R.Data/SocialCapital.RData")
load("R.Data/strength.to.deceased.RData")
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2019.txt")
#Initilize structures:
ChangeProx.BM = data.frame(); ChangeGroom.BM = data.frame()
ChangeProx.NFC = data.frame(); ChangeGroom.NFC = data.frame()
ChangeProx.CFC = data.frame(); ChangeGroom.CFC = data.frame()
ChangeProx.ALL = data.frame(); ChangeGroom.ALL = data.frame()
#Format data
SocialCapital.ALL$id = as.character(SocialCapital.ALL$id);
SocialCapital.ALL$sex = as.factor(SocialCapital.ALL$sex);
SocialCapital.ALL$age = as.numeric(SocialCapital.ALL$age)
SocialCapital.ALL$group = as.factor(SocialCapital.ALL$group)
SocialCapital.ALL$percentrank = as.numeric(SocialCapital.ALL$percentrank)/100
SocialCapital.ALL$year = as.factor(SocialCapital.ALL$year)
#Set parameters:
num_iter = 3; iter =1
only2017=F; #if only considering 2017 (year just prior hurricane). Note: there is  no V2017 valid dp(groom) & dp(prox) because they all have less than 20 obs.
group = c("KK","KK","V", "V", "V")
years = c(2015,2017,2015,2016,2017)
groupyears = c("KK2015", "KK2017","V2015", "V2016", "V2017")
dprob.ALL = data.frame();
for (iter in 1:num_iter){
print(paste("%%%%%%%%%%%%%%%%%% iter",iter, "%%%%%%%%%%%%%%%%%%"))
#####################################################################
# 1. Compute change in p(Acc) and p(Social), per individual, per year
#####################################################################
#Calculate random subsamples
randomScans = calcRandomScans(allScans)
gy=1
for (gy in 1:length(groupyears)){
rscans = randomScans[which(randomScans$year == years[gy] & randomScans$group == group[gy]),]
#Load data
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Behavioral_Data/Data All Cleaned")
meta_data = read.csv(paste("Group",groupyears[gy],"_GroupByYear.txt", sep = ""))
unqIDs = as.character(meta_data$id)
dprob=data.frame(matrix(NA, nrow=length(unqIDs),ncol=4)); colnames(dprob)=c("id","dpAcc","dpSocial","num_obs")
for (id in 1:length(unqIDs)){ #For all individuals
isProx.pre = rscans$isProx[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 0)] #get all pre-hurricane data for that individuals
isProx.post = rscans$isProx[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 1)]#get all post-re-hurricane data for that individuals
isSocial.pre = rscans$isSocial[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 0)] #get all pre-hurricane data for that individuals
isSocial.post = rscans$isSocial[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 1)]#get all post-re-hurricane data for that individuals
dpAcc=NA; dpSocial=NA; num_obs = length(isProx.pre)
if (length(isProx.pre)>=20) { #If there are more than 10 observations for that individual
pACC.pre = sum(isProx.pre)/length(isProx.pre)
pACC.post = sum(isProx.post)/length(isProx.post)
dpAcc = pACC.post - pACC.pre
pSocial.pre = sum(isSocial.pre)/length(isSocial.pre)
pSocial.post = sum(isSocial.post)/length(isSocial.post)
dpSocial = pSocial.post - pSocial.pre
} #end of min obs clause
dprob[id,]=c(unqIDs[id],dpAcc,dpSocial,num_obs)
} #end of id for loop
dprob$group = group[gy]; dprob$year = years[gy]; dprob$iter=iter
dprob.ALL = rbind(dprob.ALL, dprob)
} #end of groupyear for loop
}
library(lme4)# Generalized Linear Mixed Models
#library(lmerTest)
library(performance)
library(sjPlot)
library(jtools)
library(ggplot2)
library(dplyr)
#library(glmmTMB)# Generalized Linear Mixed Models, other package
#library(MCMCglmm)# Generalized Linear Mixed Models, other package
#library(bbmle)#Tools for General Maximum Likelihood Estimation
#library(DHARMa) #residual diagnostic fr hierarchical (multi-level/mixed) regression models
#Load data
setwd("C:/Users/Camille Testard/Documents/Github/Cayo-Maria/")
source("cleaned_code/Functions/CalcSubsampledScans.R")
load("R.Data/SocialCapital.RData")
load("R.Data/strength.to.deceased.RData")
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2019.txt")
#Format data
SocialCapital.ALL$id = as.character(SocialCapital.ALL$id);
SocialCapital.ALL$sex = as.factor(SocialCapital.ALL$sex);
SocialCapital.ALL$age = as.numeric(SocialCapital.ALL$age)
SocialCapital.ALL$group = as.factor(SocialCapital.ALL$group)
SocialCapital.ALL$percentrank = as.numeric(SocialCapital.ALL$percentrank)/100
SocialCapital.ALL$year = as.factor(SocialCapital.ALL$year)
#Set parameters:
num_iter = 3; iter =1
only2017=F; #if only considering 2017 (year just prior hurricane). Note: there is  no V2017 valid dp(groom) & dp(prox) because they all have less than 20 obs.
group = c("KK","KK","V", "V", "V")
years = c(2015,2017,2015,2016,2017)
groupyears = c("KK2015", "KK2017","V2015", "V2016", "V2017")
dprob.ALL = data.frame();
for (iter in 1:num_iter){
print(paste("%%%%%%%%%%%%%%%%%% iter",iter, "%%%%%%%%%%%%%%%%%%"))
#####################################################################
# 1. Compute change in p(Acc) and p(Social), per individual, per year
#####################################################################
#Calculate random subsamples
randomScans = calcRandomScans(allScans)
gy=1
for (gy in 1:length(groupyears)){
rscans = randomScans[which(randomScans$year == years[gy] & randomScans$group == group[gy]),]
#Load data
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Behavioral_Data/Data All Cleaned")
meta_data = read.csv(paste("Group",groupyears[gy],"_GroupByYear.txt", sep = ""))
unqIDs = as.character(meta_data$id)
dprob=data.frame(matrix(NA, nrow=length(unqIDs),ncol=4)); colnames(dprob)=c("id","dpAcc","dpSocial","num_obs")
for (id in 1:length(unqIDs)){ #For all individuals
isProx.pre = rscans$isProx[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 0)] #get all pre-hurricane data for that individuals
isProx.post = rscans$isProx[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 1)]#get all post-re-hurricane data for that individuals
isSocial.pre = rscans$isSocial[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 0)] #get all pre-hurricane data for that individuals
isSocial.post = rscans$isSocial[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 1)]#get all post-re-hurricane data for that individuals
dpAcc=NA; dpSocial=NA; num_obs = length(isProx.pre)
if (length(isProx.pre)>=20) { #If there are more than 10 observations for that individual
pACC.pre = sum(isProx.pre)/length(isProx.pre)
pACC.post = sum(isProx.post)/length(isProx.post)
dpAcc = pACC.post - pACC.pre
pSocial.pre = sum(isSocial.pre)/length(isSocial.pre)
pSocial.post = sum(isSocial.post)/length(isSocial.post)
dpSocial = pSocial.post - pSocial.pre
} #end of min obs clause
dprob[id,]=c(unqIDs[id],dpAcc,dpSocial,num_obs)
} #end of id for loop
dprob$group = group[gy]; dprob$year = years[gy]; dprob$iter=iter
dprob.ALL = rbind(dprob.ALL, dprob)
} #end of groupyear for loop
}
dprob.ALL$dpAcc=as.numeric(dprob.ALL$dpAcc)
dprob.ALL$dpSocial=as.numeric(dprob.ALL$dpSocial)
dprob.ALL = dprob.ALL[-which(is.na(dprob.ALL$dpAcc)),] #remove NA
#####################################################################
# 2. Merge dprob, social Capital and strength to dead IDs data &clean
#####################################################################
data.combined=merge.data.frame(dprob.ALL,strength.to.deceased, by=intersect(c("id","year"),c("id","year")))
data=merge.data.frame(data.combined,SocialCapital.ALL,by=intersect(c("id","year"),c("id","year")))
data$group.x=NULL; data$group.y=NULL
#Change ord rank to have two levels only
data$ordrank2 = "L"; data$ordrank2[which(data$ordrank =="H")]="H" #
#Add "isIncrease" factor for grooming. If we simply want to model whether an indvidual increased its grooming or not (binary).
data$isSocialIncrease = 0; data$isSocialIncrease[which(data$dpSocial>0)]=1
data$isSocialIncrease = as.factor(data$isSocialIncrease)
#Scale parameters:
data[,c("age","DSIgroom","numPartnersGroom")] <- scale(data[,c("age","DSIgroom","numPartnersGroom")])
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Results/PreHurricaneFactors-BehavioralFlex/Plots")
#check distribution of independent variables
# tiff("ChangeProb.tiff",units="in", width=7, height=4, res=300, compression = 'lzw')
hist(data$dpSocial,col=rgb(0,1,1,0.5), breaks=20,main="Change p(Prox/Groom) pre- to post-hurricane",xlab="Change p(Prox/Groom)", xlim=c(-0.5,1))
hist(data$dpAcc,col=rgb(1,0,0,0.5), breaks=30,add=T)
segments(x0=0,y0=0,x1=0,y1=3000,col="red",lwd=4, lty=2)
box()
legend("topright", c("Change p(Prox)", "Change p(Groom)"), fill=c("red", "cyan"))
# dev.off()
ggplot(data, aes(x=dead.all, y=dpSocial))+
geom_point(alpha = 0.25, size = 3) +
geom_smooth(method=lm, color=rgb(1,0,0))+
ggtitle("Post-disaster change in grooming as a function of pre-disaster strength of bond to deceased IDs")+
xlab("Strength of bond to deceased IDs pre-disaster (standardized)")+
ylab("")+
facet_grid(~sex)
cor.test(data$dpSocial,data$dead.all) #test correlation
ggplot(data, aes(x=dead.all, y=dpSocial))+
geom_point(alpha = 0.25, size = 3) +
geom_smooth(method=lm, color=rgb(1,0,0))+
ggtitle("Post-disaster change in grooming as a function of pre-disaster strength of bond to deceased IDs")+
xlab("Strength of bond to deceased IDs pre-disaster (standardized)")+
ylab("Change in Grooming")
data[,c("age","DSIgroom","numPartnersGroom","dead.all")] <- scale(data[,c("age","DSIgroom","numPartnersGroom")])
data[,c("age","DSIgroom","numPartnersGroom","dead.all")] <- scale(data[,c("age","DSIgroom","numPartnersGroom","dead.all")])
ggplot(data, aes(x=dead.all, y=dpSocial))+
geom_point(alpha = 0.25, size = 3) +
geom_smooth(method=lm, color=rgb(1,0,0))+
ggtitle("Post-disaster change in grooming as a function of pre-disaster strength of bond to deceased IDs")+
xlab("Strength of bond to deceased IDs pre-disaster (standardized)")+
ylab("Change in Grooming")
cor.test(data$dpSocial,data$dead.all) #test correlation
dpSocial <- lmer(dpSocial~ sex + age + group + percentrank + dead.all + (1|id) +(1|year), data = data, na.action=na.omit)
summary(dpSocial)
dpSocial2 <- lmer(dpSocial~ sex + age + group + percentrank + dead.all +DSIgroom + numPartnersGroom +  (1|year), data = data, na.action=na.omit)
summary(dpSocial2)
dead<-ggplot(data, aes(x=dead.all, y=dpSocial))+
geom_point(alpha = 0.25, size = 3) +
geom_smooth(method=lm, color=rgb(1,0,0))+
ggtitle("Post-disaster change in grooming as a function of pre-disaster strength of bond to deceased IDs")+
xlab("Strength of bond to deceased IDs pre-disaster (standardized)")+
ylab("Change in Grooming")
cor.test(data$dpSocial,data$dead.all) #test correlation
tiff("dead.tiff",units="in", width=7, height=4, res=300, compression = 'lzw'); dead; dev.off() #save plot
tiff("dead.tiff",units="in", width=8, height=4, res=300, compression = 'lzw'); dead; dev.off() #save plot
tiff("dead.tiff",units="in", width=8.5, height=4, res=300, compression = 'lzw'); dead; dev.off() #save plot
dpSocial2 <- lmer(dpSocial~ sex + age + group + percentrank + dead.all +DSIgroom + numPartnersGroom +  (1|year), data = data, na.action=na.omit)
summary(dpSocial2)
isSocialIncrease <- glmer(isSocialIncrease ~ sex + age + group + percentrank +  (1|year), data = data, na.action=na.omit, family = binomial)
summary(isSocialIncreas
e)
summary(isSocialIncrease)
export_summs(dpSocial2, digits = 4, to.file = "docx", file.name = "ModelingChangePSocPreGroom.AllYears.docx")
dpSocial2 <- lmer(dpSocial~ sex + age + group + percentrank + dead.all +DSIgroom + numPartnersGroom +(1|id)+  (1|year), data = data, na.action=na.omit)
summary(dpSocial2)
export_summs(dpSocial2, digits = 4, to.file = "docx", file.name = "ModelingChangePSocPreGroom.AllYears.docx")
export_summs(dpSocial2, digits = 4, to.file = "docx", file.name = "ModelingChangePSocPreGroom.AllYears.docx")
dpSocial <- lmer(dpSocial~ sex + age + group + percentrank + dead.all + (1|id) +(1|year), data = data, na.action=na.omit)
summary(dpSocial)
######## Modelling Logistic Regressions
library(lme4)# Generalized Linear Mixed Models
#library(lmerTest)
library(performance)
library(sjPlot)
library(jtools)
library(ggplot2)
library(dplyr)
#library(glmmTMB)# Generalized Linear Mixed Models, other package
#library(MCMCglmm)# Generalized Linear Mixed Models, other package
#library(bbmle)#Tools for General Maximum Likelihood Estimation
#library(DHARMa) #residual diagnostic fr hierarchical (multi-level/mixed) regression models
#Load data
setwd("C:/Users/Camille Testard/Documents/Github/Cayo-Maria/")
source("cleaned_code/Functions/CalcSubsampledScans.R")
load("R.Data/SocialCapital.RData")
load("R.Data/strength.to.deceased.RData")
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/")
allScans = read.csv("Behavioral_Data/Data All Cleaned/allScans2019.txt")
#Format data
SocialCapital.ALL$id = as.character(SocialCapital.ALL$id);
SocialCapital.ALL$sex = as.factor(SocialCapital.ALL$sex);
SocialCapital.ALL$age = as.numeric(SocialCapital.ALL$age)
SocialCapital.ALL$group = as.factor(SocialCapital.ALL$group)
SocialCapital.ALL$percentrank = as.numeric(SocialCapital.ALL$percentrank)/100
SocialCapital.ALL$year = as.factor(SocialCapital.ALL$year)
#Set parameters:
num_iter = 100; iter =1
only2017=F; #if only considering 2017 (year just prior hurricane). Note: there is  no V2017 valid dp(groom) & dp(prox) because they all have less than 20 obs.
group = c("KK","KK","V", "V", "V")
years = c(2015,2017,2015,2016,2017)
groupyears = c("KK2015", "KK2017","V2015", "V2016", "V2017")
dprob.ALL = data.frame();
for (iter in 1:num_iter){
print(paste("%%%%%%%%%%%%%%%%%% iter",iter, "%%%%%%%%%%%%%%%%%%"))
#####################################################################
# 1. Compute change in p(Acc) and p(Social), per individual, per year
#####################################################################
#Calculate random subsamples
randomScans = calcRandomScans(allScans)
gy=1
for (gy in 1:length(groupyears)){
rscans = randomScans[which(randomScans$year == years[gy] & randomScans$group == group[gy]),]
#Load data
setwd("C:/Users/Camille Testard/Desktop/Desktop-Cayo-Maria/Behavioral_Data/Data All Cleaned")
meta_data = read.csv(paste("Group",groupyears[gy],"_GroupByYear.txt", sep = ""))
unqIDs = as.character(meta_data$id)
dprob=data.frame(matrix(NA, nrow=length(unqIDs),ncol=4)); colnames(dprob)=c("id","dpAcc","dpSocial","num_obs")
for (id in 1:length(unqIDs)){ #For all individuals
isProx.pre = rscans$isProx[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 0)] #get all pre-hurricane data for that individuals
isProx.post = rscans$isProx[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 1)]#get all post-re-hurricane data for that individuals
isSocial.pre = rscans$isSocial[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 0)] #get all pre-hurricane data for that individuals
isSocial.post = rscans$isSocial[which(as.character(rscans$focalID) == unqIDs[id] & rscans$isPost == 1)]#get all post-re-hurricane data for that individuals
dpAcc=NA; dpSocial=NA; num_obs = length(isProx.pre)
if (length(isProx.pre)>=20) { #If there are more than 10 observations for that individual
pACC.pre = sum(isProx.pre)/length(isProx.pre)
pACC.post = sum(isProx.post)/length(isProx.post)
dpAcc = pACC.post - pACC.pre
pSocial.pre = sum(isSocial.pre)/length(isSocial.pre)
pSocial.post = sum(isSocial.post)/length(isSocial.post)
dpSocial = pSocial.post - pSocial.pre
} #end of min obs clause
dprob[id,]=c(unqIDs[id],dpAcc,dpSocial,num_obs)
} #end of id for loop
dprob$group = group[gy]; dprob$year = years[gy]; dprob$iter=iter
dprob.ALL = rbind(dprob.ALL, dprob)
} #end of groupyear for loop
}
dprob.ALL$dpAcc=as.numeric(dprob.ALL$dpAcc)
dprob.ALL$dpSocial=as.numeric(dprob.ALL$dpSocial)
dprob.ALL = dprob.ALL[-which(is.na(dprob.ALL$dpAcc)),] #remove NA
#####################################################################
# 2. Merge dprob, social Capital and strength to dead IDs data &clean
#####################################################################
data.combined=merge.data.frame(dprob.ALL,strength.to.deceased, by=intersect(c("id","year"),c("id","year")))
data=merge.data.frame(data.combined,SocialCapital.ALL,by=intersect(c("id","year"),c("id","year")))
data$group.x=NULL; data$group.y=NULL
setwd("C:/Users/Camille Testard/Documents/Github/Cayo-Maria/R.Data")
save(data,file="FactorsPredictChangeP.RData")
setwd("C:/Users/Camille Testard/Documents/Github/Cayo-Maria/R.Data")
save(dprob.ALL,file="ChangeP.RData")
